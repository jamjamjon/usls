{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p> Get Started  Model Zoo  API Reference</p>"},{"location":"#usls","title":"usls","text":"<p>A cross-platform Rust library powered by ONNX Runtime for efficient inference of SOTA vision and vision-language models.</p> <p></p>"},{"location":"#features","title":"\u2728 Features","text":"<ul> <li> <p> High Performance</p> <p>Multi-threading, SIMD, and CUDA-accelerated processing with ONNX Runtime execution providers</p> </li> <li> <p> Cross-Platform</p> <p>Linux, macOS, Windows with support for CUDA, TensorRT, CoreML, OpenVINO, DirectML</p> </li> <li> <p> Unified API</p> <p>Single <code>Model</code> trait with <code>run()</code>/<code>forward()</code> and unified <code>Y</code> output for all models</p> </li> <li> <p> Auto-Management</p> <p>Automatic model download from HuggingFace/GitHub, caching and path resolution</p> </li> <li> <p> Multiple Inputs</p> <p>Support for images, directories, videos, webcam, streams and combinations</p> </li> <li> <p> Precision Support</p> <p>FP32, FP16, INT8, UINT8, Q4, Q4F16, BNB4, and more quantization options</p> </li> <li> <p> Full-Stack Suite</p> <p>Complete workflows with <code>DataLoader</code>, <code>Annotator</code>, and <code>Viewer</code></p> </li> <li> <p> Model Ecosystem</p> <p>50+ SOTA vision and VLM models ready to use</p> </li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Project Status</p> <p>This is a personal project maintained in spare time. Progress on performance optimization and new model support may vary.</p>"},{"location":"contributing/#areas-for-contribution","title":"Areas for Contribution","text":"Area Description Model Optimization Optimize model interfaces and post-processing New Models Add support for new vision/VLM models Bug Fixes Fix issues and edge cases Documentation Improve docs and examples Other Any other improvements or suggestions"},{"location":"contributing/#pull-requests","title":"Pull Requests","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Run <code>cargo fmt</code> and <code>cargo clippy</code></li> <li>Submit PR with clear description</li> </ol> <p>Questions?</p> <p>Open a discussion or issue.</p>"},{"location":"faq/","title":"FAQ","text":"ONNX Runtime Issues <p>Check these issue trackers:</p> <ul> <li>ort </li> <li>onnxruntime</li> </ul> Linking errors with <code>__isoc23_strtoll</code>? <p>Set the ORT_DYLIB_PATH environment variable to the path to libonnxruntime.so/onnxruntime.dll <pre><code>export ORT_DYLIB_PATH=/path/to/onnxruntime/lib/\n</code></pre></p> <p>Use the dynamic loading feature: <pre><code>export ORT_DYLIB_PATH=/path/to/onnxruntime/lib/\ncargo run -F ort-load-dynamic --example\n</code></pre></p> Other Linking Errors? <p>See ORT Linking for more information.</p> Why no LLM models? <ul> <li>Focus: Vision and VLM models under 1B parameters</li> <li>LLM inference engines like vLLM already exist</li> <li>Pure text embedding models may be added in the future</li> </ul> How fast is it? <ul> <li>YOLO benchmarks: see Performance</li> <li>Optimizations: multi-threading, SIMD, CUDA acceleration</li> <li>YOLO and RFDETR are well-optimized; other models may need more work</li> </ul> <p>Still have questions?</p> <p>Open a GitHub Issue.</p>"},{"location":"cargo-features/image-formats/","title":"Image Formats","text":"Feature Format Description Default <code>image-gif</code> GIF Graphics Interchange Format x <code>image-bmp</code> BMP Windows Bitmap x <code>image-ico</code> ICO Windows Icon x <code>image-avif</code> AVIF AV1 Image File Format x <code>image-tiff</code> TIFF Tagged Image File Format x <code>image-dds</code> DDS DirectDraw Surface x <code>image-exr</code> OpenEXR High dynamic-range imaging x <code>image-ff</code> Farbfeld Simple lossless format x <code>image-hdr</code> Radiance HDR x <code>image-pnm</code> PNM Portable Anymap (PPM/PGM/PBM) x <code>image-qoi</code> QOI Quite OK Image format x <code>image-tga</code> TGA Truevision Targa x <code>image-all-formats</code> - Enable all optional formats at once x <p>Default Support</p> <p><code>jpeg</code>, <code>png</code>, <code>webp</code>, <code>rayon</code> are enabled by default (via <code>image</code> crate).</p> <p>Usage Example</p> <pre><code># Enable optional formats\nusls = { version = \"0.1\", features = [\"image-avif\", \"image-tiff\"] }\n</code></pre>"},{"location":"cargo-features/models/","title":"Model Categories","text":"Feature Category Models Dependencies Default <code>vision</code> Core Vision Detection, Segmentation, Classification, Pose, OBB - \u2713 <code>vlm</code> Vision-Language CLIP, BLIP, Florence2, multi-modal <code>tokenizers</code>, <code>ndarray-npy</code> x <code>mot</code> Tracking Multi-Object Tracking utilities - x <code>all-models</code> All vision + vlm + mot combined <code>tokenizers</code>, <code>ndarray-npy</code> x <p>Recommended Setup</p> <pre><code># Standard computer vision\nfeatures = [\"vision\"]\n\n# With vision-language capabilities\nfeatures = [\"vision\", \"vlm\"]\n\n# Everything included\nfeatures = [\"all-models\"]\n</code></pre>"},{"location":"cargo-features/ort/","title":"Execution Providers","text":"<p>Hardware acceleration for inference. Enable the one matching your hardware.</p>"},{"location":"cargo-features/ort/#onnx-runtime","title":"ONNX Runtime","text":"Feature Description Default <code>ort-download-binaries</code> Auto-download ONNX Runtime binaries from pyke \u2713 <code>ort-load-dynamic</code> Manual linking for custom builds. See Linking Guide x"},{"location":"cargo-features/ort/#execution-providers_1","title":"Execution Providers","text":"Feature Platform Description <code>cuda</code> NVIDIA GPU CUDA execution provider <code>tensorrt</code> NVIDIA GPU TensorRT execution provider <code>nvrtx</code> NVIDIA GPU NVRTX execution provider <code>coreml</code> Apple Silicon macOS/iOS inference <code>openvino</code> Intel CPU/GPU/VPU acceleration <code>directml</code> Windows DirectML acceleration <code>rocm</code> AMD GPU ROCm acceleration <code>onednn</code> Intel Deep Neural Network Library <code>cann</code> Huawei Ascend NPU <code>rknpu</code> Rockchip NPU acceleration <code>armnn</code> ARM Neural Network SDK <code>xnnpack</code> Mobile CPU optimization <code>webgpu</code> Web WebGPU/Chrome <code>nnapi</code> Android Neural Networks API <code>qnn</code> Qualcomm SNPE acceleration <code>tvm</code> - Apache TVM <code>azure</code> Azure ML execution provider <code>migraphx</code> AMD MIGraphX <code>vitis</code> Xilinx Vitis AI"},{"location":"cargo-features/ort/#cuda-image-processor","title":"CUDA Image Processor","text":"<p>Prerequisites</p> <p>Requires cudarc for CUDA kernels.</p> <p>Enable GPU-accelerated image preprocessing:</p> Pattern Description Example <code>&lt;ep&gt;-full</code> Auto-detect CUDA version via <code>nvcc</code> <code>cuda-full</code>, <code>tensorrt-full</code> <code>&lt;ep&gt;-cuda-&lt;ver&gt;</code> Specific CUDA version <code>cuda-12040</code>, <code>tensorrt-cuda-12040</code> <ul> <li><code>&lt;ep&gt;</code>: <code>cuda</code>, <code>tensorrt</code>, or <code>nvrtx</code></li> <li><code>&lt;ver&gt;</code>: Specific CUDA version</li> </ul>"},{"location":"cargo-features/ort/#supported-cuda-versions","title":"Supported CUDA Versions","text":"Version Features 11.x <code>cuda-11040</code>, <code>cuda-11050</code>, <code>cuda-11060</code>, <code>cuda-11070</code>, <code>cuda-11080</code> 12.x <code>cuda-12000</code>, <code>cuda-12010</code>, <code>cuda-12020</code>, <code>cuda-12030</code>, <code>cuda-12040</code>, <code>cuda-12050</code>, <code>cuda-12060</code>, <code>cuda-12080</code>, <code>cuda-12090</code> 13.x <code>cuda-13000</code>, <code>cuda-13010</code> <p>TensorRT/NVRTX Versions</p> <p>Replace <code>cuda-</code> with <code>tensorrt-cuda-</code> or <code>nvrtx-cuda-</code> for TensorRT/NVRTX versions. Example: <code>tensorrt-cuda-12040</code>, <code>nvrtx-cuda-12080</code></p>"},{"location":"cargo-features/ort/#feature-device-combinations","title":"Feature &amp; Device Combinations","text":"Scenario Feature Model Device Processor Speed CPU Only <code>vision</code> (default) <code>cpu</code> <code>cpu</code> Baseline CUDA <code>cuda</code> <code>cuda</code> <code>cpu</code> Slow preprocess CUDA (fast) <code>cuda-full</code> <code>cuda</code> <code>cuda</code> Fast preprocess TensorRT <code>tensorrt</code> <code>tensorrt</code> <code>cpu</code> Slow preprocess TensorRT (fast) <code>tensorrt-full</code> <code>tensorrt</code> <code>cuda</code> Fast preprocess <p>TensorRT EP + CUDA EP + CUDA Image Processor</p> <pre><code>features = [\"tensorrt-full\", \"cuda\"]\n# Or\nfeatures = [\"tensorrt\", \"cuda-full\"]\n</code></pre> <p>Device Consistency</p> <p>Different EPs can use different devices (e.g., <code>tensorrt:0</code> + <code>cuda:1</code>).</p> <p>However, when using NVIDIA EP + CUDA image processor, they MUST use the same GPU ID: <pre><code># \u2705 Correct: same GPU\n--device cuda:0 --processor-device cuda:0\n\n# \u274c Wrong: different GPUs\n--device cuda:0 --processor-device cuda:1\n</code></pre></p> <p>Don't mix CUDA versions</p> <pre><code># \u274c Wrong\nfeatures = [\"cuda-12040\", \"cuda-11080\"]\n\n# \u2705 Correct\nfeatures = [\"tensorrt-full\"]\n</code></pre>"},{"location":"cargo-features/overview/","title":"Cargo Features","text":"<p>usls is highly modular. Use feature flags to include only the models and hardware support you need, keeping your binary small and compilation fast. </p> <ul> <li> <p> ORT &amp; Execution Providers</p> <p>ONNX Runtime, Hardware acceleration: CUDA, TensorRT, CoreML, OpenVINO, and more.</p> <p> Providers \u2192</p> </li> <li> <p> Utilities Features</p> <p>visualization, video I/O, model hub, and annotation utilities.</p> <p> Utilities \u2192</p> </li> <li> <p> Image Formats</p> <p>Additional image format support for faster compilation.</p> <p> Image Formats \u2192</p> </li> <li> <p> Model Categories</p> <p>Vision models, Vision-Language Models, and tracking utilities.</p> <p> Models \u2192</p> </li> </ul>"},{"location":"cargo-features/utils/","title":"Utilities","text":"Feature Category Description Dependencies Default <code>annotator</code> Annotation Draw bounding boxes, keypoints, masks on images <code>ab_glyph</code>, <code>imageproc</code> \u2713 <code>viewer</code> Visualization Real-time image/video display (like OpenCV <code>imshow</code>) <code>minifb</code> x <code>video</code> I/O Video read/write streaming support <code>video-rs</code> x <code>hf-hub</code> Model Hub Download models from Hugging Face <code>hf-hub</code> x <p>Usage Example</p> <pre><code># Default: annotation only\nusls = \"0.1\"\n\n# With viewer and video\nusls = { version = \"0.1\", features = [\"viewer\", \"video\"] }\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>To use usls in your project, add it to your <code>Cargo.toml</code>.</p> GitHub (Recommended)Crates.io <pre><code>[dependencies]\nusls = { git = \"https://github.com/jamjamjon/usls\", branch = \"main\" }\n</code></pre> <pre><code>[dependencies]\nusls = { version = \"latest-version\", features = [ \"cuda\" ] }\n</code></pre> <p>Available Cargo Features Selection</p> <p>usls provides multiple Cargo features for different execution providers and capabilities.</p> <p>Select the features that match your hardware and use case.</p> <p> View All Cargo Features \u2192</p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Integration</p> <p>Learn how to use usls in your code</p> <p>Integrate \u2192</p> </li> </ul>"},{"location":"getting-started/integration/","title":"Integration Workflow","text":"<p><code>usls</code> implements a clean, modular pipeline from data ingestion to results visualization.</p>"},{"location":"getting-started/integration/#the-4-step-pipeline","title":"The 4-Step Pipeline","text":"<ol> <li>Configure Model: Select a pre-configured model (e.g., <code>Config::rfdetr_nano()</code>), customize settings, and commit the configuration.</li> <li>Load Data: Setup a <code>DataLoader</code> to handle your input sources (images, videos, etc.).</li> <li>Inference: Iterate through the <code>DataLoader</code> and pass data to <code>model.run()</code> or <code>model.forward()</code>.</li> <li>Extract Results: Access detections, masks, or embeddings from the unified <code>Y</code> output.</li> <li>Annotate (Optional): Use the <code>Annotator</code> to draw results back onto the original images.</li> <li>Visualize (Optional): Use the <code>Viewer</code> for real-time display or video recording.</li> </ol> <p>Example</p> <pre><code>use usls::*;\n\nfn main() -&gt; anyhow::Result&lt;()&gt; {\n    // 1. Configure &amp; Build Model\n    let config = Config::rfdetr_nano()\n        .with_model_device(Device::Cuda(0))\n        .commit()?;\n    let mut model = RFDETR::new(config)?;\n\n    // 2. Setup DataLoader\n    let dl = DataLoader::new(\"image.jpg\")?\n        .with_batch(model.batch())\n        .stream()?;\n\n    // optional: Annotate\n    let annotator = Annotator::default();\n\n    // optional: Viewer\n    let mut viewer = Viewer::default();\n\n    // 3. Run Inference\n    for xs in dl {\n        let ys = model.run(&amp;xs)?;\n        for (x, y) in xs.iter().zip(ys.iter()) {\n            // 4. Access results\n            for hb in y.hbbs() {\n                println!(\"{}\", hb);\n            }\n\n            // optional: Check if the window is closed and exit if so.\n            if viewer.is_window_exist_and_closed() {\n                break;\n            }\n\n            // optional: Annotate\n            let image_annotated = annotator.annotate(x, y)?;\n\n            // optional: Display the current image.\n            viewer.imshow(&amp;image_annotated)?;\n\n            // optional: Wait for a key press or timeout, and exit on Escape.\n            if let Some(key) = viewer.wait_key(10) {\n                if key == usls::Key::Escape {\n                    break;\n                }\n            }\n\n            // optional: Save the annotated image.\n            image_annotated.save(\"output.jpg\")?;\n        }\n    }\n    Ok(())\n}\n</code></pre>"},{"location":"getting-started/integration/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Guides</p> <p>Learn more about modules, config, and advanced usage</p> <p>Read Guides \u2192</p> </li> <li> <p> Model Zoo</p> <p>Explore 50+ pre-trained models</p> <p>Browse Models \u2192</p> </li> <li> <p> FAQ</p> <p>Find answers to common questions</p> <p>View FAQ \u2192</p> </li> </ul>"},{"location":"getting-started/overview/","title":"Getting Started","text":"<p>Welcome to usls! </p> <p>This guide will help you get up and running with the library in just a few minutes.</p>"},{"location":"getting-started/overview/#start-with-yolo-demo","title":"\ud83d\ude80 Start with YOLO Demo","text":"<p>Let's run the YOLO-Series demo to explore models with different tasks, precision and execution providers:</p> <ul> <li>Tasks: <code>detect</code>, <code>segment</code>, <code>pose</code>, <code>classify</code>, <code>obb</code></li> <li>Versions: <code>5</code>, <code>6</code>, <code>7</code>, <code>8</code>, <code>9</code>, <code>10</code>, <code>11</code>, <code>12</code>, <code>13</code>, <code>26</code></li> <li>Scales: <code>n</code>, <code>s</code>, <code>m</code>, <code>l</code>, <code>x</code></li> <li>Precision (DType): <code>fp32</code>, <code>fp16</code>, <code>q8</code>, <code>q4</code>, <code>q4f16</code>, <code>bnb4</code></li> <li>Devices: <code>cpu</code>, <code>cuda:0</code>, <code>tensorrt:0</code>, <code>coreml</code>, <code>openvino:CPU</code></li> </ul> <p>First, clone the repository and navigate to the project root</p> <pre><code>git clone https://github.com/jamjamjon/usls.git\ncd usls\n</code></pre> <p>Then, run the demo:</p> CPU (Default)NVIDIA GPU (CUDA)NVIDIA GPU (TensorRT)Apple Silicon (CoreML) <pre><code># Object detection with YOLO26n (FP16)\ncargo run -r --example yolo -- --task detect --ver 26 --scale n --dtype fp16\n</code></pre> <pre><code># Requires \"cuda-full\" feature\ncargo run -r -F cuda-full --example yolo -- --task segment --ver 11 --scale m --device cuda:0 --processor-device cuda:0\n</code></pre> <pre><code># Requires \"tensorrt-full\" feature\ncargo run -r -F tensorrt-full --example yolo -- --device tensorrt:0 --processor-device cuda:0\n</code></pre> <pre><code># Requires \"coreml\" feature\ncargo run -r -F coreml --example yolo -- --device coreml\n</code></pre> <p>For a full list of options, run:</p> <pre><code>cargo run -r --example yolo -- --help\n</code></pre>"},{"location":"getting-started/overview/#performance-reference","title":"\ud83d\udcca Performance Reference","text":"<p>Environment: NVIDIA RTX 3060Ti (CUDA 12.8) / Intel i5-12400F Setup: YOLO26n, 640x640 resolution, COCO2017 val set (5,000 images)</p> EP ImageProcessor DType Batch Preprocess Inference Postprocess Total TensorRT CUDA FP16 1 ~233\u00b5s ~1.3ms ~14\u00b5s ~1.55ms TensorRT-RTX CUDA FP32 1 ~233\u00b5s ~2.0ms ~10\u00b5s ~2.24ms TensorRT-RTX CUDA FP16 1 \u2753 \u2753 \u2753 \u2753 CUDA CUDA FP32 1 ~233\u00b5s ~5.0ms ~17\u00b5s ~5.25ms CUDA CUDA FP16 1 ~233\u00b5s ~3.6ms ~17\u00b5s ~3.85ms CUDA CPU FP32 1 ~800\u00b5s ~6.5ms ~14\u00b5s ~7.31ms CUDA CPU FP16 1 ~800\u00b5s ~5.0ms ~14\u00b5s ~5.81ms CPU CPU FP32 1 ~970\u00b5s ~20.5ms ~14\u00b5s ~21.48ms CPU CPU FP16 1 ~970\u00b5s ~25.0ms ~14\u00b5s ~25.98ms TensorRT CUDA FP16 8 ~1.2ms ~6.0ms ~55\u00b5s ~7.26ms TensorRT CPU FP16 8 ~18.0ms ~25.5ms ~55\u00b5s ~43.56ms <p>Multi-Batch Performance</p> <p>When using a larger batch size (e.g., batch 8), CUDA Image processor significantly improves throughput on GPUs.</p>"},{"location":"getting-started/overview/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Installation</p> <p>Install usls in your own project</p> <p>Install \u2192</p> </li> <li> <p> Integration</p> <p>Learn how to integrate usls into your code</p> <p>Integrate \u2192</p> </li> </ul>"},{"location":"guides/annotator/","title":"Annotator","text":"<p>Visualizes model results with customizable styles.</p> <p>Quick Start</p> <pre><code>use usls::*;\n\nlet annotator = Annotator::default();\nlet ys = model.run(&amp;images)?;\n\nfor (img, y) in images.iter().zip(ys.iter()) {\n    annotator.annotate(img, y)?.save(\"output.jpg\")?;\n}\n</code></pre>"},{"location":"guides/annotator/#supported-types","title":"Supported Types","text":"Type Style Key Methods HBB <code>HbbStyle</code> <code>thickness</code>, <code>draw_fill</code>, <code>draw_outline</code>, <code>mode</code> OBB <code>ObbStyle</code> <code>thickness</code>, <code>draw_fill</code>, <code>mode</code> Keypoint <code>KeypointStyle</code> <code>radius</code>, <code>skeleton</code>, <code>skeleton_thickness</code>, <code>mode</code> Polygon <code>PolygonStyle</code> <code>thickness</code>, <code>draw_fill</code>, <code>background_overlay</code> Mask <code>MaskStyle</code> <code>mode</code> (<code>Overlay</code>/<code>Halo</code>), <code>draw_polygons</code> Prob <code>ProbStyle</code> <code>text_x_pos</code>, <code>text_y_pos</code>"},{"location":"guides/annotator/#hbbstyle","title":"HbbStyle","text":"<p>Modes: <code>Solid</code> (default), <code>Dashed</code>, <code>Corners</code>, <code>Rounded</code></p> <p>Example</p> <pre><code>use usls::viz::*;\n\n// Dashed box\nlet style = HbbStyle::dashed()\n    .with_thickness(3)\n    .with_mode(HbbStyleMode::Dashed { length: 15, gap: 8 });\n\n// Corners only\nlet style = HbbStyle::corners()\n    .with_mode(HbbStyleMode::Corners { ratio_long: 0.25, ratio_short: 0.15 });\n</code></pre>"},{"location":"guides/annotator/#keypointstyle","title":"KeypointStyle","text":"<p>Modes: <code>Circle</code> (default), <code>Star</code>, <code>Square</code>, <code>Diamond</code>, <code>Triangle</code>, <code>Cross</code>, <code>X</code>, <code>RoundedSquare</code>, <code>Glow</code></p> <p>Example</p> <pre><code>use usls::viz::*;\n\n// With skeleton\nlet style = KeypointStyle::default()\n    .with_skeleton(SKELETON_COCO_19.into())\n    .with_radius(6)\n    .with_skeleton_thickness(2);\n\n// Star shape\nlet style = KeypointStyle::star()\n    .with_mode(KeypointStyleMode::Star { points: 6, inner_ratio: 0.4 });\n</code></pre>"},{"location":"guides/annotator/#maskstyle","title":"MaskStyle","text":"<p>Modes: <code>Overlay</code> (default), <code>Halo</code></p> <p>Example</p> <pre><code>use usls::viz::*;\n\n// Halo effect\nlet style = MaskStyle::halo()\n    .with_mode(MaskStyleMode::halo_with(0.08, Color::magenta().with_alpha(200)));\n\n// With contours\nlet style = MaskStyle::default()\n    .with_draw_polygon_largest(true)\n    .with_draw_hbbs(true);\n</code></pre>"},{"location":"guides/annotator/#textstyle","title":"TextStyle","text":"<p>Locations: <code>OuterTopLeft</code>, <code>OuterTopRight</code>, <code>InnerTopLeft</code>, <code>InnerTopRight</code>, <code>Center</code>, etc.</p> <p>Modes: <code>Rect(padding)</code>, <code>Rounded(padding, radius)</code></p> <p>Example</p> <pre><code>use usls::viz::*;\n\nlet style = TextStyle::default()\n    .with_loc(TextLoc::InnerTopLeft)\n    .with_mode(TextStyleMode::rounded(5.0, 3.0))\n    .with_confidence(true)\n    .with_name(true)\n    .with_id(false);\n</code></pre>"},{"location":"guides/annotator/#colorsource","title":"ColorSource","text":"Source Description <code>Auto</code> Auto from palette <code>AutoAlpha(u8)</code> Auto with custom alpha <code>InheritOutline</code> Inherit shape outline color <code>InheritFill</code> Inherit shape fill color <code>Custom(Color)</code> Specific color <p>Example</p> <pre><code>use usls::viz::*;\n\nHbbStyle::default()\n    .with_outline_color(ColorSource::Custom(Color::red()))\n    .with_fill_color(ColorSource::Custom(Color::red().with_alpha(60)));\n</code></pre>"},{"location":"guides/annotator/#complete-example","title":"Complete Example","text":"<p>Custom Annotator</p> <pre><code>use usls::*;\nuse usls::viz::*;\n\nfn main() -&gt; anyhow::Result&lt;()&gt; {\n    let config = Config::default().commit()?;\n    let mut model = YOLO::new(config)?;\n\n    let annotator = Annotator::default()\n        .with_hbb_style(\n            HbbStyle::default()\n                .with_thickness(2)\n                .with_outline_color(Color::Green)\n        )\n        .with_text_style(\n            TextStyle::default()\n                .with_color(Color::White)\n                .with_bg_fill_color(Color::Black.with_alpha(0.5))\n        );\n\n    let dl = DataLoader::new(\"image.jpg\")?.stream()?;\n\n    for (images, _) in dl {\n        let ys = model.run(&amp;images)?;\n        for (img, y) in images.iter().zip(ys.iter()) {\n            annotator.annotate(img, y)?.save(\"output.jpg\")?;\n        }\n    }\n\n    Ok(())\n}\n</code></pre>"},{"location":"guides/annotator/#per-instance-override","title":"Per-Instance Override","text":"<p>Each result can have its own style:</p> <p>Example</p> <pre><code>use usls::viz::*;\n\nHbb::default()\n    .with_xyxy(100.0, 100.0, 300.0, 200.0)\n    .with_name(\"custom\")\n    .with_style(\n        HbbStyle::corners()\n            .with_thickness(6)\n            .with_outline_color(ColorSource::Custom(Color::magenta()))\n    );\n</code></pre>"},{"location":"guides/config/","title":"Configuration and Module System","text":"<p>Core Concepts</p> <p>usls uses two core concepts:</p> <ul> <li>Module System for organizing model components</li> <li>Config System for configuring everything through a unified builder API.</li> </ul> <p>API Naming Convention</p> <p>The <code>Config</code> system in usls follows a builder pattern with a strict naming convention, making it easy to discover and use APIs.</p> <ol> <li>Per-Module: <ul> <li><code>with_&lt;module_name&gt;_&lt;field_name&gt;(&lt;value&gt;)</code></li> <li><code>with_module_&lt;field_name&gt;(&lt;module_name&gt;, &lt;value&gt;)</code></li> </ul> </li> <li>Global: <ul> <li><code>with_&lt;field_name&gt;_all(&lt;value&gt;)</code></li> </ul> </li> </ol>"},{"location":"guides/config/#module-system","title":"Module System","text":"<p>Models can be composed of multiple ONNX components (modules), each configurable independently.</p> <p>Why Modules?</p> <ul> <li>Flexible Device Placement: Each module can run on different devices (GPU/CPU) independently</li> <li>Flexible Precision: Set FP16, FP32, or INT8 per module as needed</li> <li>Flexible EP Settings: Configure execution provider options per module</li> </ul>"},{"location":"guides/config/#available-modules","title":"Available Modules","text":"Module Purpose Example Models <code>Model</code> Default single-module YOLO, RT-DETR <code>Visual</code> / <code>Textual</code> Vision-language components CLIP, BLIP <code>Encoder</code> / <code>Decoder</code> Generic components Seq2seq models <code>VisualEncoder</code> / <code>TextualEncoder</code> / <code>TextualDecoder</code><code>VisualProjection</code> / <code>TextualProjection</code> VLMs Multi-modal <code>SizeEncoder</code> / <code>SizeDecoder</code><code>CoordEncoder</code> / <code>CoordDecoder</code> Size encoding Specialized models"},{"location":"guides/config/#usage-examples","title":"Usage Examples","text":"<pre><code>// Single-module model\nlet config = Config::yolo()\n    .with_model_device(Device::Cuda(0))\n    .commit()?;\n\n// Multi-module model (CLIP)\nlet config = Config::clip()\n    .with_visual_device(Device::Cuda(0))   // GPU for images\n    .with_textual_device(Device::Cpu)      // CPU for text\n    .commit()?;\n</code></pre>"},{"location":"guides/config/#config-system","title":"Config System","text":""},{"location":"guides/config/#model-basics","title":"Model Basics","text":"<p>Configure model identity and auto-naming (YOLO-specific):</p> Field Method Description <code>name</code> <code>with_name(\"yolo\")</code> Model identifier <code>task</code> <code>with_task(Task::ObjectDetection)</code> Detection/Seg/Pose/Obb <code>version</code> <code>with_version(Version::V8)</code> YOLO version <code>scale</code> <code>with_scale(Scale::N)</code> Model size (N/S/M/L/X) <p>YOLO Auto-Naming</p> <p>For YOLO models, the ONNX filename is auto-generated: <code>{version}-{scale}-{task}.onnx</code> <pre><code>Config::yolo()\n    .with_version(Version::V8)\n    .with_scale(Scale::N)\n    .with_task(Task::ObjectDetection)\n    // Auto-generates: v8-n-det.onnx\n</code></pre></p>"},{"location":"guides/config/#onnx-runtime-config","title":"ONNX Runtime Config","text":"<p>Configure inference engine settings per module:</p> Field Method Description <code>file</code> <code>with_model_file(\"path.onnx\")</code> ONNX model file path <code>device</code> <code>with_model_device(Device::Cuda(0))</code> Execution provider device <code>dtype</code> <code>with_model_dtype(DType::Fp16)</code> Precision (FP32/FP16/INT8/etc) <code>batch</code> <code>with_model_batch(8)</code> Static batch size <code>batch_min_opt_max</code> <code>with_model_batch_min_opt_max(1, 4, 8)</code> Dynamic batch range <code>ixx</code> <code>with_model_ixx(0, 2, (416, 640, 800))</code> Dynamic tensor dimensions <code>num_dry_run</code> <code>with_model_num_dry_run(3)</code> Warmup iterations <code>graph_opt_level</code> <code>with_model_graph_opt_level(3)</code> Graph optimization (0-3) <code>num_intra_threads</code> <code>with_model_num_intra_threads(4)</code> Intra-op parallelism <code>num_inter_threads</code> <code>with_model_num_inter_threads(4)</code> Inter-op parallelism <p>Dynamic Shapes (TensorRT)</p> <pre><code>Config::yolo()\n    .with_model_device(Device::TensorRT(0))\n    .with_model_ixx(0, 0, (1, 1, 8))        // Batch: min/opt/max\n    .with_model_ixx(0, 2, (416, 640, 800))  // Height\n    .with_model_ixx(0, 3, (416, 640, 800))  // Width\n    .commit()?;\n</code></pre>"},{"location":"guides/config/#image-processor-config","title":"Image Processor Config","text":"<p>Configure preprocessing pipeline:</p> Field Method Description <code>device</code> <code>with_image_processor_device(Device::Cuda(0))</code> Preprocessing device <code>mean</code> <code>with_image_mean([0.485, 0.456, 0.406])</code> Normalization mean <code>std</code> <code>with_image_std([0.229, 0.224, 0.225])</code> Normalization std <code>resize_mode</code> <code>with_resize_mode_type(ResizeModeType::Stretch)</code> Resize strategy <code>resize_alg</code> <code>with_resize_alg(ResizeAlg::Nearest)</code> Resize algorithm <code>normalize</code> <code>with_normalize(true)</code> Enable normalization <code>padding_value</code> <code>with_padding_value(114)</code> Pad value (0-255) <code>do_resize</code> <code>with_do_resize(true)</code> Enable resizing <code>pad_image</code> <code>with_pad_image(true)</code> Pad for super-resolution <code>pad_size</code> <code>with_pad_size(64)</code> Pad alignment size"},{"location":"guides/config/#text-processor-config-vlm","title":"Text Processor Config (VLM)","text":"<p>Configure text/tokenization settings:</p> Field Method Description <code>tokenizer_file</code> <code>with_tokenizer_file(\"tokenizer.json\")</code> Tokenizer path <code>tokenizer_config_file</code> <code>with_tokenizer_config_file(\"config.json\")</code> Tokenizer config <code>special_tokens_map_file</code> <code>with_special_tokens_map_file(\"special_tokens.json\")</code> Special tokens <code>config_file</code> <code>with_config_file(\"config.json\")</code> Model config <code>model_max_length</code> <code>with_model_max_length(2048)</code> Max token length"},{"location":"guides/config/#inference-parameters","title":"Inference Parameters","text":"<p>Configure model-specific inference settings:</p> Category Field Method Default Detection <code>class_names</code> <code>with_class_names(vec![\"person\"])</code> <code>[]</code> <code>class_confs</code> <code>with_class_confs(&amp;[0.25])</code> <code>[0.25]</code> <code>iou</code> <code>with_iou(0.45)</code> <code>None</code> <code>apply_nms</code> <code>with_apply_nms(true)</code> <code>None</code> <code>topk</code> <code>with_topk(100)</code> <code>None</code> <code>classes_excluded</code> <code>exclude_classes(&amp;[0])</code> <code>[]</code> <code>classes_retained</code> <code>retain_classes(&amp;[0])</code> <code>[]</code> <code>min_width</code> <code>with_min_width(10.0)</code> <code>None</code> <code>min_height</code> <code>with_min_height(10.0)</code> <code>None</code> Keypoint <code>keypoint_names</code> <code>with_keypoint_names(vec![\"nose\"])</code> <code>[]</code> <code>keypoint_confs</code> <code>with_keypoint_confs(&amp;[0.35])</code> <code>[0.35]</code> Segmentation <code>num_masks</code> <code>with_num_masks(32)</code> <code>None</code> <code>find_contours</code> <code>with_find_contours(true)</code> <code>false</code> OCR/Text <code>text_names</code> <code>with_text_names(vec![\"text\"])</code> <code>[]</code> <code>text_confs</code> <code>with_text_confs(&amp;[0.25])</code> <code>[0.25]</code> <code>db_unclip_ratio</code> <code>with_db_unclip_ratio(1.5)</code> <code>1.5</code> <code>db_binary_thresh</code> <code>with_db_binary_thresh(0.2)</code> <code>0.2</code> <code>max_tokens</code> <code>with_max_tokens(256)</code> <code>None</code> <code>token_level_class</code> <code>with_token_level_class(true)</code> <code>false</code> Common <code>apply_softmax</code> <code>with_apply_softmax(true)</code> <code>false</code>"},{"location":"guides/config/#pre-configured-models","title":"Pre-configured Models","text":"<p>All preset configs are in <code>src/models/</code>. Each model has a <code>config.rs</code> with defaults. </p> <p>Examples are in <code>examples/</code>.</p>"},{"location":"guides/config/#full-example","title":"Full Example","text":"<p>Complete Configuration</p> <pre><code>use usls::*;\n\nfn main() -&gt; anyhow::Result&lt;()&gt; {\n    let config = Config::yolo()\n        // Model basics\n        .with_task(Task::ObjectDetection)\n        .with_version(Version::V8)\n        .with_scale(Scale::N)\n\n        // ONNX Runtime config\n        .with_model_device(Device::TensorRT(0))\n        .with_model_dtype(DType::Fp16)\n        .with_model_ixx(0, 0, (1, 4, 8))        // Batch\n        .with_model_ixx(0, 2, (416, 640, 800))  // Height\n        .with_model_ixx(0, 3, (416, 640, 800))  // Width\n\n        // Image processor\n        .with_processor_device(Device::Cuda(0))\n        .with_normalize(true)\n\n        // Inference params\n        .with_class_confs(&amp;[0.35])\n        .with_iou(0.45)\n        .commit()?;\n\n    let mut model = YOLO::new(config)?;\n    Ok(())\n}\n</code></pre> <p>Always Call .commit()</p> <p>Configuration is validated and finalized when <code>.commit()?</code> is called.</p>"},{"location":"guides/dataloader/","title":"Data Loading","text":"<p>The <code>DataLoader</code> provides efficient, batched data ingestion from multiple sources with automatic memory management.</p> <p>Key Features</p> <ul> <li>Multi-source \u2014 Images, videos, webcams, URLs, directories, globs</li> <li>Batching \u2014 Automatic batch collation with padding</li> <li>Streaming \u2014 Background thread for non-blocking iteration</li> <li>Progress \u2014 Built-in progress bars for long operations</li> </ul>"},{"location":"guides/dataloader/#supported-sources","title":"Supported Sources","text":"Source Example Description Single image <code>\"image.jpg\"</code> Load one image file Directory <code>\"./assets/\"</code> All images in directory Glob pattern <code>\"./assets/*.jpg\"</code> Pattern matching Video file <code>\"video.mp4\"</code> Video frames as images Webcam <code>\"0\"</code>, <code>\"1\"</code> Camera device index RTSP stream <code>\"rtsp://...\"</code> Network camera stream URL <code>\"https://.../img.jpg\"</code> Remote image Mixed <code>vec![\"a.jpg\", \"b.png\"]</code> or <code>\"a.jpg | video.mp4\"</code> Multiple sources <p>Source Combinations</p> <pre><code>// Multiple sources at once\nlet dl = DataLoader::new(vec![\n    \"./images/*.jpg\",\n    \"./videos/sample.mp4\",\n    \"https://example.com/image.png\",\n    \"0\",  // Webcam\n])?;\n\n// Or use a string, separated by `|`\nlet dl = DataLoader::new(\"a.jpg | video.mp4\")?;\n</code></pre>"},{"location":"guides/dataloader/#usage-patterns","title":"Usage Patterns","text":""},{"location":"guides/dataloader/#1-direct-reading-small-data","title":"1. Direct Reading (Small Data)","text":"<p>Best for</p> <p>Small datasets that fit in memory</p> <pre><code>use usls::DataLoader;\n\n// Single image\nlet image = DataLoader::new(\"image.jpg\")?.try_read_one()?;\n\n// Nth image from collection\nlet image = DataLoader::new(\"./images/*.jpg\")?.try_read_nth(2)?;\n\n// Range of images\nlet images = DataLoader::new(\"./assets\")?.try_read_range(0..5)?;\n\n// All images\nlet images = DataLoader::new(\"./assets/*.png\")?.try_read()?;\n</code></pre>"},{"location":"guides/dataloader/#2-streaming-large-datavideos","title":"2. Streaming (Large Data/Videos)","text":"<p>Best for</p> <p>Videos, webcams, or large datasets</p> <pre><code>let dl = DataLoader::new(\"video.mp4\")?\n    .with_batch(32)           // 32 images per batch\n    .with_progress_bar(true)  // Show progress\n    .stream()?;               // Start background thread\n\nfor (i, (images, paths)) in dl.into_iter().enumerate() {\n    println!(\"Batch {}: {} images\", i, images.len());\n    // Process batch...\n}\n</code></pre>"},{"location":"guides/dataloader/#configuration","title":"Configuration","text":"Method Description Default <code>with_batch(n)</code> Batch size <code>1</code> <code>with_nfv_skip(n)</code> Skip n frames for video stream / webcam <code>0</code> <code>with_progress_bar(b)</code> Show terminal progress <code>true</code> <p>Video with frame skipping</p> <pre><code>let dl = DataLoader::new(\"video.mp4\")?\n    .with_batch(1)\n    .with_nfv_skip(2)  // Skip 2 frames\n    .stream()?;\n</code></pre>"},{"location":"guides/dataloader/#integration-with-models","title":"Integration with Models","text":"<p>Complete Pipeline</p> <pre><code>use usls::*;\n\nfn main() -&gt; anyhow::Result&lt;()&gt; {\n    // Setup model\n    let config = Config::yolo()\n        .with_model_device(Device::Cuda(0))\n        .commit()?;\n    let mut model = YOLO::new(config)?;\n\n    // Setup dataloader\n    let dl = DataLoader::new(\"image.jpg\")?\n        .with_batch(model.batch())  // Use model's batch size\n        .stream()?;\n\n    // Inference loop\n    for images in dl {\n        let results = model.run(&amp;images)?;\n    }\n\n    Ok(())\n}\n</code></pre>"},{"location":"guides/device/","title":"Device Management","text":"<p>usls provides flexible device control, allowing you to run the model (inference) and processor (preprocessing) on different devices for optimal performance.</p> <p>Key Concept</p> Component Description Example Device Model Neural network inference <code>Device::Cuda(0)</code>, <code>Device::TensorRT(0)</code> Image Processor Image preprocessing (resize, normalize) <code>Device::Cpu</code>, <code>Device::Cuda(0)</code>"},{"location":"guides/device/#supported-devices","title":"Supported Devices","text":"Device Description Required Feature <code>Device::Cpu</code> Standard CPU inference (default) <code>Device::Cuda(id)</code> NVIDIA GPU via CUDA <code>cuda</code> or <code>cuda-full</code> <code>Device::TensorRT(id)</code> NVIDIA GPU via TensorRT <code>tensorrt</code> or <code>tensorrt-full</code> <code>Device::NvTensorRT(id)</code> NVIDIA RTX via TensorRT-RTX <code>nvrtx</code> or <code>nvrtx-full</code> <code>Device::CoreML</code> Apple Silicon (macOS/iOS) <code>coreml</code> <code>Device::OpenVINO(id)</code> Intel CPU/GPU/VPU <code>openvino</code> <code>Device::DirectML(id)</code> Windows DirectML <code>directml</code> <p>Feature Flags</p> <p>Add to your <code>Cargo.toml</code>: <pre><code>[dependencies]\nusls = { git = \"...\", features = [\"cuda\"] }\n</code></pre></p>"},{"location":"guides/device/#device-combinations","title":"Device Combinations","text":"Scenario Model Device Processor Device Feature Flag CPU Only <code>Device::Cpu</code> <code>Device::Cpu</code> (none) GPU Inference <code>Device::Cuda(0)</code> <code>Device::Cpu</code> <code>cuda</code> GPU Full <code>Device::Cuda(0)</code> <code>Device::Cuda(0)</code> <code>cuda-full</code> TensorRT <code>Device::TensorRT(0)</code> <code>Device::Cpu</code> <code>tensorrt</code> TensorRT Full <code>Device::TensorRT(0)</code> <code>Device::Cuda(0)</code> <code>tensorrt-full</code> TensorRT-RTX <code>Device::NvTensorRT(0)</code> <code>Device::Cpu</code> <code>nvrtx</code> TensorRT-RTX Full <code>Device::NvTensorRT(0)</code> <code>Device::Cuda(0)</code> <code>nvrtx-full</code> Apple Silicon <code>Device::CoreML</code> <code>Device::Cpu</code> <code>coreml</code> <p>GPU Consistency</p> <p>In multi-GPU environments, ensure both model and processor use the same GPU ID: <pre><code>// CORRECT: Both on GPU 0\nconfig.with_model_device(Device::Cuda(0))\n      .with_processor_device(Device::Cuda(0));\n\n// INCORRECT: Different GPUs cause data transfer overhead\nconfig.with_model_device(Device::Cuda(0))\n      .with_processor_device(Device::Cuda(1));\n</code></pre></p>"},{"location":"guides/device/#configuration-examples","title":"Configuration Examples","text":""},{"location":"guides/device/#cpu-only","title":"CPU Only","text":"<p>Example</p> <pre><code>let config = Config::default()\n    .with_model_device(Device::Cpu)\n    .with_processor_device(Device::Cpu)\n    .commit()?;\n</code></pre>"},{"location":"guides/device/#gpu-inference-cpu-preprocessing","title":"GPU Inference (CPU Preprocessing)","text":"<p>Example</p> <pre><code>let config = Config::default()\n    .with_model_device(Device::Cuda(0))\n    .with_processor_device(Device::Cpu)\n    .commit()?;\n</code></pre>"},{"location":"guides/device/#gpu-full-gpu-preprocessing","title":"GPU Full (GPU Preprocessing)","text":"<p>Example</p>"},{"location":"guides/device/#cuda-with-cuda-preprocessing","title":"CUDA with CUDA Preprocessing","text":"<pre><code>let config = Config::default()\n    .with_model_device(Device::Cuda(0))\n    .with_processor_device(Device::Cuda(0))\n    .commit()?;\n</code></pre>"},{"location":"guides/device/#tensorrt-with-cuda-preprocessing","title":"TensorRT with CUDA Preprocessing","text":"<pre><code>let config = Config::default()\n    .with_model_device(Device::TensorRT(0))\n    .with_processor_device(Device::Cuda(0))\n    .commit()?;\n</code></pre>"},{"location":"guides/device/#multi-gpu-support","title":"Multi-GPU Support","text":"<p>Selecting GPU</p> <p>Use device index to select specific GPU:</p> <pre><code>// Use GPU 1\nlet config = Config::default()\n    .with_model_device(Device::Cuda(1))\n    .with_processor_device(Device::Cuda(1))\n    .commit()?;\n\n// Use GPU 2 with TensorRT\nlet config = Config::default()\n    .with_model_device(Device::TensorRT(2))\n    .with_processor_device(Device::Cuda(2))\n    .commit()?;\n</code></pre>"},{"location":"guides/dtype/","title":"Data Types (DType)","text":"<p>usls supports multiple precision levels to balance accuracy, speed, and memory usage.</p> <p>Quick Reference</p> DType Precision Speed Memory Best For <code>Fp32</code> 32-bit float Baseline 100% Maximum accuracy <code>Fp16</code> 16-bit float 2-3x faster 50% Modern GPUs <code>Q8</code> 8-bit quantized Fast 25% Edge deployment <code>Q4F16</code> 4-bit + FP16 Fast ~15% VLMs, limited memory <code>Bnb4</code> BitsAndBytes 4-bit Fast ~12% Ultra-low memory"},{"location":"guides/dtype/#configuration","title":"Configuration","text":""},{"location":"guides/dtype/#global-all-modules","title":"Global (All Modules)","text":"<p>Example</p> <pre><code>let config = Config::sam3()\n    .with_dtype_all(DType::Fp16)\n    .commit()?;\n</code></pre>"},{"location":"guides/dtype/#per-module","title":"Per-Module","text":"<p>Example</p> <pre><code>let config = Config::clip()\n    // Fast visual encoding\n    .with_visual_dtype(DType::Fp16)\n    // Accurate text encoding\n    .with_textual_dtype(DType::Fp32)\n    .commit()?;\n</code></pre>"},{"location":"guides/dtype/#tensorrt-note","title":"TensorRT Note","text":"<p>TensorRT FP32 Behavior</p> <p>TensorRT automatically converts FP32 to FP16 for performance. Use <code>--dtype fp32</code> with TensorRT for optimal speed.</p> <p>TensorRT-RTX preserves the input precision exactly.</p>"},{"location":"guides/ep/","title":"Execution Providers","text":"<p>Execution Providers (EPs) enable hardware-accelerated inference. All EPs support the configuration pattern: <code>with_&lt;module&gt;_&lt;ep&gt;_&lt;option&gt;()</code> / <code>with_&lt;ep&gt;_&lt;option&gt;_all()</code>.</p> <p>Quick Reference</p> Provider Feature Flag Device Best For TensorRT <code>tensorrt</code> <code>Device::TensorRT(id)</code> NVIDIA GPUs (fastest) TensorRT-RTX <code>nvrtx</code> <code>Device::NvTensorRT(id)</code> RTX GPUs CUDA <code>cuda</code> <code>Device::Cuda(id)</code> NVIDIA GPUs CoreML <code>coreml</code> <code>Device::CoreML</code> Apple Silicon OpenVINO <code>openvino</code> <code>Device::OpenVINO(target)</code> Intel CPUs/GPUs DirectML <code>directml</code> <code>Device::DirectML(id)</code> Windows MIGraphX <code>migraphx</code> <code>Device::MIGraphX</code> AMD GPUs CANN <code>cann</code> <code>Device::CANN(id)</code> Huawei Ascend oneDNN <code>onednn</code> <code>Device::OneDNN</code> Intel CPUs NNAPI <code>nnapi</code> <code>Device::NNAPI</code> Android ARM NN <code>armnn</code> <code>Device::ArmNN</code> ARM devices WebGPU <code>webgpu</code> <code>Device::WebGPU</code> Browsers"},{"location":"guides/ep/#tensorrt","title":"TensorRT","text":"Option Type Default Description <code>fp16</code> <code>bool</code> <code>true</code> Enable FP16 precision <code>engine_cache</code> <code>bool</code> <code>true</code> Cache compiled engines <code>timing_cache</code> <code>bool</code> <code>false</code> Cache timing profiles <code>builder_optimization_level</code> <code>u8</code> <code>3</code> Builder optimization (0-5) <code>max_workspace_size</code> <code>usize</code> <code>1073741824</code> Max workspace (1GB) <code>min_subgraph_size</code> <code>usize</code> <code>1</code> Min subgraph node count <code>dump_ep_context_model</code> <code>bool</code> <code>false</code> Dump context model <code>dump_subgraphs</code> <code>bool</code> <code>false</code> Dump subgraphs <p>Example</p> <pre><code>Config::default()\n    .with_model_device(Device::TensorRT(0))\n    .with_model_tensorrt_fp16(true)\n    .with_model_tensorrt_engine_cache(true)\n    .with_model_tensorrt_builder_optimization_level(3)\n    .commit()?;\n</code></pre> <p>First Run Slow</p> <p>TensorRT builds engines on first run. Enable <code>engine_cache</code> for instant subsequent loads.</p>"},{"location":"guides/ep/#dynamic-shapes","title":"Dynamic Shapes","text":"<p>Dynamic shapes in <code>usls</code> are configured in a way that closely mirrors <code>trtexec</code>.</p> <p><code>trtexec</code> example:</p> <p>Example</p> <pre><code>trtexec --fp16 --onnx=your_model.onnx \\\n    --minShapes=images:1x3x416x416 \\\n    --optShapes=images:1x3x640x640 \\\n    --maxShapes=images:8x3x800x800 \\\n    --saveEngine=your_model.engine\n</code></pre> <p>Equivalent <code>usls</code> configuration:</p> <p>Example</p> <pre><code>Config::default()\n    .with_model_ixx(0, 0, (1, 1, 8))        // batch: min=1, opt=1, max=8\n    .with_model_ixx(0, 1, 3)                // channels: fixed at 3\n    .with_model_ixx(0, 2, (416, 640, 800))  // height: min/opt/max\n    .with_model_ixx(0, 3, (416, 640, 800))  // width: min/opt/max\n    .commit()?;\n</code></pre>"},{"location":"guides/ep/#tensorrt-rtx","title":"TensorRT-RTX","text":"<p>Same options as TensorRT, but preserves input precision (no auto FP32\u2192FP16 conversion).</p> <p>Example</p> <pre><code>Config::default()\n    .with_model_device(Device::NvTensorRT(0))\n    .commit()?;\n</code></pre> <p>TensorRT vs TensorRT-RTX</p> <ul> <li>TensorRT EP: Automatically handles FP32\u2192FP16 conversion. Use <code>--dtype fp32</code> for optimal performance.</li> <li>TensorRT-RTX EP: Preserves input precision. No automatic conversion.</li> </ul>"},{"location":"guides/ep/#cuda","title":"CUDA","text":"Option Type Default Description <code>cuda_graph</code> <code>bool</code> <code>false</code> Enable CUDA graph capture <code>fuse_conv_bias</code> <code>bool</code> <code>false</code> Fuse conv+bias for perf <code>conv_max_workspace</code> <code>bool</code> <code>true</code> Max workspace for conv search <code>tf32</code> <code>bool</code> <code>true</code> Enable TF32 on Ampere+ <code>prefer_nhwc</code> <code>bool</code> <code>true</code> Prefer NHWC layout <p>Example</p> <pre><code>Config::default()\n    .with_model_device(Device::Cuda(0))\n    .with_model_cuda_cuda_graph(true)\n    .with_model_cuda_tf32(true)\n    .commit()?;\n</code></pre>"},{"location":"guides/ep/#coreml-apple","title":"CoreML (Apple)","text":"Option Type Default Description <code>static_input_shapes</code> <code>bool</code> <code>true</code> Static shapes for optimization <code>subgraph_running</code> <code>bool</code> <code>true</code> Enable subgraph mode <code>model_format</code> <code>u8</code> <code>0</code> <code>0</code>=MLProgram, <code>1</code>=NeuralNetwork <code>compute_units</code> <code>u8</code> <code>0</code> <code>0</code>=All, <code>1</code>=CPUAndGPU, <code>2</code>=CPUAndNeuralEngine, <code>3</code>=CPUOnly <code>specialization_strategy</code> <code>u8</code> <code>1</code> <code>0</code>=Default, <code>1</code>=FastPrediction, <code>2</code>=FastCompilation <p>Example</p> <pre><code>Config::default()\n    .with_model_device(Device::CoreML)\n    .with_model_coreml_static_input_shapes(true)\n    .with_model_coreml_compute_units(0)\n    .commit()?;\n</code></pre>"},{"location":"guides/ep/#openvino-intel","title":"OpenVINO (Intel)","text":"Option Type Default Description <code>dynamic_shapes</code> <code>bool</code> <code>true</code> Enable dynamic shapes <code>opencl_throttling</code> <code>bool</code> <code>true</code> Enable OpenCL throttling <code>qdq_optimizer</code> <code>bool</code> <code>true</code> Enable QDQ optimizer <code>num_threads</code> <code>usize</code> <code>8</code> Number of threads <p>Example</p> <pre><code>// CPU target\nConfig::default()\n    .with_model_device(Device::OpenVINO(\"CPU\".to_string()))\n    .with_model_openvino_num_threads(8)\n    .commit()?;\n\n// GPU target\nConfig::default()\n    .with_model_device(Device::OpenVINO(\"GPU\".to_string()))\n    .commit()?;\n</code></pre> <p>Dynamic Loading</p> <p>Some platforms require: <code>cargo run -F openvino -F ort-load-dynamic</code></p>"},{"location":"guides/ep/#onednn-intel","title":"oneDNN (Intel)","text":"Option Type Default Description <code>arena_allocator</code> <code>bool</code> <code>true</code> Enable arena allocator <p>Example</p> <pre><code>Config::default()\n    .with_model_device(Device::OneDNN)\n    .with_model_onednn_arena_allocator(true)\n    .commit()?;\n</code></pre>"},{"location":"guides/ep/#cann-huawei","title":"CANN (Huawei)","text":"Option Type Default Description <code>graph_inference</code> <code>bool</code> <code>true</code> Enable graph inference <code>dump_graphs</code> <code>bool</code> <code>false</code> Dump graphs for debug <code>dump_om_model</code> <code>bool</code> <code>true</code> Dump OM model <p>Example</p> <pre><code>Config::default()\n    .with_model_device(Device::CANN(0))\n    .with_model_cann_graph_inference(true)\n    .commit()?;\n</code></pre>"},{"location":"guides/ep/#migraphx-amd","title":"MIGraphX (AMD)","text":"Option Type Default Description <code>fp16</code> <code>bool</code> <code>true</code> Enable FP16 precision <code>exhaustive_tune</code> <code>bool</code> <code>false</code> Exhaustive tuning <p>Example</p> <pre><code>Config::default()\n    .with_model_device(Device::MIGraphX)\n    .with_model_migraphx_fp16(true)\n    .commit()?;\n</code></pre>"},{"location":"guides/ep/#nnapi-android","title":"NNAPI (Android)","text":"Option Type Default Description <code>cpu_only</code> <code>bool</code> <code>false</code> Force CPU-only execution <code>disable_cpu</code> <code>bool</code> <code>false</code> Disable CPU fallback <code>fp16</code> <code>bool</code> <code>true</code> Enable FP16 precision <code>nchw</code> <code>bool</code> <code>false</code> Use NCHW layout <p>Example</p> <pre><code>Config::default()\n    .with_model_device(Device::NNAPI)\n    .with_model_nnapi_fp16(true)\n    .commit()?;\n</code></pre>"},{"location":"guides/ep/#arm-nn","title":"ARM NN","text":"Option Type Default Description <code>arena_allocator</code> <code>bool</code> <code>true</code> Enable arena allocator <p>Example</p> <pre><code>Config::default()\n    .with_model_device(Device::ArmNN)\n    .with_model_armnn_arena_allocator(true)\n    .commit()?;\n</code></pre>"},{"location":"guides/ep/#webgpu","title":"WebGPU","text":"<p>No configurable parameters currently.</p> <p>Example</p> <pre><code>Config::default()\n    .with_model_device(Device::WebGPU)\n    .commit()?;\n</code></pre>"},{"location":"guides/ep/#cpu","title":"CPU","text":"Option Type Default Description <code>arena_allocator</code> <code>bool</code> <code>true</code> Enable arena allocator <p>Example</p> <pre><code>Config::default()\n    .with_model_device(Device::Cpu)\n    .with_model_cpu_arena_allocator(true)\n    .commit()?;\n</code></pre>"},{"location":"guides/ep/#configuration-patterns","title":"Configuration Patterns","text":"Pattern Method Scope Per-module <code>with_model_&lt;ep&gt;_&lt;option&gt;()</code> Single module Global <code>with_&lt;ep&gt;_&lt;option&gt;_all()</code> All modules Explicit <code>with_&lt;ep&gt;_&lt;option&gt;_module(Module, value)</code> Specific module <p>Example</p> <pre><code>Config::default()\n    // TensorRT FP16 for model module only\n    .with_model_tensorrt_fp16(true)\n\n    // CoreML static shapes for all modules\n    .with_coreml_static_input_shapes_all(true)\n\n    // Explicit module specification\n    .with_tensorrt_fp16_module(Module::Visual, true)\n    .commit()?;\n</code></pre>"},{"location":"guides/overview/","title":"Guides","text":"<p>Welcome to the usls guides! This section provides comprehensive documentation for understanding and using the library effectively.</p>"},{"location":"guides/overview/#core-concepts","title":"Core Concepts","text":"<ul> <li> <p> Module System</p> <p>Understand how models are organized into modules (Model, Visual, Textual, Encoder, Decoder)</p> <p>Learn More \u2192</p> </li> <li> <p> Configuration</p> <p>Learn the builder-pattern API for configuring models with devices, dtypes, and execution providers</p> <p>Learn More \u2192</p> </li> <li> <p> Data Loading</p> <p>Load images, videos, webcams, and streams efficiently with automatic batching</p> <p>Learn More \u2192</p> </li> <li> <p> Results &amp; Annotation</p> <p>Work with unified <code>Y</code> results and visualize with <code>Annotator</code> and <code>Viewer</code></p> <p>Results \u2192 \u00b7 Annotator \u2192 \u00b7 Viewer \u2192</p> </li> <li> <p> Execution Providers</p> <p>CUDA, TensorRT, CoreML, OpenVINO, DirectML, and more</p> <p>Learn More \u2192</p> </li> <li> <p> Device Management</p> <p>Configure model and processor devices separately for optimal performance</p> <p>Learn More \u2192</p> </li> <li> <p> Data Types</p> <p>FP32, FP16, INT8, Q8, Q4F16, BNB4 precision support</p> <p>Learn More \u2192</p> </li> </ul>"},{"location":"guides/results/","title":"Results","text":"<p>All models return results in a unified <code>Y</code> structure.</p> <p>Quick Start</p> <pre><code>let ys: Vec&lt;Y&gt; = model.run(&amp;images)?;\n</code></pre>"},{"location":"guides/results/#y-structure","title":"Y Structure","text":"Field Type Description <code>hbbs</code> <code>Vec&lt;Hbb&gt;</code> Horizontal bounding boxes <code>obbs</code> <code>Vec&lt;Obb&gt;</code> Oriented bounding boxes <code>masks</code> <code>Vec&lt;Mask&gt;</code> Segmentation masks <code>polygons</code> <code>Vec&lt;Polygon&gt;</code> Contours <code>keypoints</code> <code>Vec&lt;Keypoint&gt;</code> Keypoints <code>keypointss</code> <code>Vec&lt;Vec&lt;Keypoint&gt;&gt;</code> Multiple keypoint sets <code>probs</code> <code>Vec&lt;Prob&gt;</code> Classification probabilities <code>texts</code> <code>Vec&lt;Text&gt;</code> OCR/VLM text <code>embedding</code> <code>X</code> Feature embeddings <code>extra</code> <code>HashMap&lt;String, X&gt;</code> Model-specific data <code>images</code> <code>Vec&lt;Image&gt;</code> Output images"},{"location":"guides/results/#common-methods","title":"Common Methods","text":"<p>All result types have metadata methods via <code>impl_meta_methods!</code>:</p> Method Returns Description <code>id()</code> <code>Option&lt;usize&gt;</code> Class ID <code>name()</code> <code>Option&lt;&amp;str&gt;</code> Class name <code>confidence()</code> <code>Option&lt;f32&gt;</code> Confidence score <code>track_id()</code> <code>Option&lt;usize&gt;</code> Tracking ID <code>uid()</code> <code>usize</code> Unique instance ID"},{"location":"guides/results/#hbb-horizontal-bounding-box","title":"Hbb (Horizontal Bounding Box)","text":""},{"location":"guides/results/#geometry-methods","title":"Geometry Methods","text":"Method Returns Description <code>xmin()</code> / <code>ymin()</code> <code>f32</code> Top-left corner <code>xmax()</code> / <code>ymax()</code> <code>f32</code> Bottom-right corner <code>cx()</code> / <code>cy()</code> <code>f32</code> Center coordinates <code>width()</code> / <code>height()</code> <code>f32</code> Box dimensions <code>xyxy()</code> <code>(f32, f32, f32, f32)</code> [x1, y1, x2, y2] <code>xywh()</code> <code>(f32, f32, f32, f32)</code> [x, y, w, h] <code>cxywh()</code> <code>(f32, f32, f32, f32)</code> [cx, cy, w, h] <code>cxcyah()</code> <code>(f32, f32, f32, f32)</code> [cx, cy, ar, h] <code>area()</code> <code>f32</code> Box area <code>perimeter()</code> <code>f32</code> Box perimeter <code>is_square()</code> <code>bool</code> Check if square"},{"location":"guides/results/#operations","title":"Operations","text":"Method Description <code>intersect(&amp;other)</code> Intersection area <code>union(&amp;other)</code> Union area <code>iou(&amp;other)</code> IoU ratio <code>contains(&amp;other)</code> Contains check <code>to_polygon()</code> Convert to Polygon"},{"location":"guides/results/#constructors","title":"Constructors","text":"Method Description <code>from_xyxy(x1, y1, x2, y2)</code> From corners <code>from_xywh(x, y, w, h)</code> From position + size <code>from_cxcywh(cx, cy, w, h)</code> From center + size <p>Example</p> <pre><code>for hbb in y.hbbs() {\n    let [x1, y1, x2, y2] = [hbb.xmin(), hbb.ymin(), hbb.xmax(), hbb.ymax()];\n    println!(\"{}: {:.2}%\", hbb.name().unwrap_or(\"?\"), hbb.confidence().unwrap_or(0.0) * 100.0);\n}\n</code></pre>"},{"location":"guides/results/#obb-oriented-bounding-box","title":"Obb (Oriented Bounding Box)","text":""},{"location":"guides/results/#geometry-methods_1","title":"Geometry Methods","text":"Method Returns Description <code>coords()</code> <code>&amp;[[f32; 2]; 4]</code> 4 vertices (CCW) <code>area()</code> <code>f32</code> Polygon area <code>top()</code> / <code>left()</code> <code>[f32; 2]</code> Extreme points <code>bottom()</code> / <code>right()</code> <code>[f32; 2]</code> Extreme points <code>is_hbb()</code> <code>bool</code> Check axis-aligned"},{"location":"guides/results/#operations_1","title":"Operations","text":"Method Description <code>intersect(&amp;other)</code> Intersection area (Sutherland-Hodgman) <code>union(&amp;other)</code> Union area <code>iou(&amp;other)</code> IoU ratio <code>to_polygon()</code> Convert to Polygon"},{"location":"guides/results/#constructors_1","title":"Constructors","text":"Method Description <code>from_cxcywhd(cx, cy, w, h, degrees)</code> Center + size + angle (deg) <code>from_cxcywhr(cx, cy, w, h, radians)</code> Center + size + angle (rad)"},{"location":"guides/results/#mask","title":"Mask","text":"Method Returns Description <code>width()</code> / <code>height()</code> <code>u32</code> Dimensions <code>dimensions()</code> <code>(u32, u32)</code> (w, h) <code>to_vec()</code> <code>Vec&lt;u8&gt;</code> Raw data <code>polygon()</code> <code>Option&lt;Polygon&gt;</code> Largest contour <code>polygons()</code> <code>Vec&lt;Polygon&gt;</code> All contours <p>Example</p> <pre><code>for mask in y.masks() {\n    println!(\"Mask: {}x{}\", mask.width(), mask.height());\n    if let Some(poly) = mask.polygon() {\n        println!(\"Area: {}\", poly.area());\n    }\n}\n</code></pre>"},{"location":"guides/results/#polygon","title":"Polygon","text":"Method Returns Description <code>count()</code> <code>usize</code> Number of points <code>points()</code> <code>Vec&lt;[f32; 2]&gt;</code> Clone of coords <code>exterior()</code> <code>&amp;[[f32; 2]]</code> Coords slice <code>is_closed()</code> <code>bool</code> Check if closed <code>area()</code> <code>f64</code> Shoelace formula <code>perimeter()</code> <code>f64</code> Euclidean length <code>centroid()</code> <code>Option&lt;(f32, f32)&gt;</code> Center point"},{"location":"guides/results/#operations_2","title":"Operations","text":"Method Description <code>intersect(&amp;other)</code> Intersection area <code>union(&amp;other)</code> Union area <code>hbb()</code> Bounding box <code>obb()</code> Minimum rotated rect <code>convex_hull()</code> Convex hull <code>simplify(eps)</code> RDP simplification <code>resample(n)</code> Add points on edges <code>unclip(delta, w, h)</code> Expand polygon"},{"location":"guides/results/#keypoint","title":"Keypoint","text":"Method Returns Description <code>x()</code> / <code>y()</code> <code>f32</code> Coordinates <code>xy()</code> <code>(f32, f32)</code> (x, y) tuple <code>is_origin()</code> <code>bool</code> Check (0, 0) <code>distance_from(&amp;other)</code> <code>f32</code> Euclidean distance <code>distance_from_origin()</code> <code>f32</code> Distance from (0,0) <code>rotate(rad)</code> <code>Self</code> Rotate around origin <code>cross(&amp;other)</code> <code>f32</code> Cross product"},{"location":"guides/results/#operators","title":"Operators","text":"<p><code>+</code>, <code>-</code>, <code>*</code>, <code>/</code> with <code>f32</code> or <code>Keypoint</code></p>"},{"location":"guides/results/#prob-classification","title":"Prob (Classification)","text":"Method Description <code>new_probs(probs, names, k)</code> Create top-k probs"},{"location":"guides/results/#text-ocrvlm","title":"Text (OCR/VLM)","text":"Method Returns Description <code>text()</code> <code>&amp;str</code> Text content"},{"location":"guides/results/#access-patterns","title":"Access Patterns","text":""},{"location":"guides/results/#borrow-reference","title":"Borrow (Reference)","text":"<p>Example</p> <pre><code>let ys = model.run(&amp;xs)?;\n\nfor y in &amp;ys {\n    for hbb in y.hbbs() {\n        println!(\"Box: {:?}, Conf: {:?}\", hbb.xyxy(), hbb.confidence());\n    }\n}\n</code></pre>"},{"location":"guides/results/#consume-ownership","title":"Consume (Ownership)","text":"<p>Example</p> <pre><code>let ys = model.run(&amp;xs)?;\n\nfor y in ys {\n    let boxes: Vec&lt;Hbb&gt; = y.hbbs;\n}\n</code></pre>"},{"location":"guides/visualization/","title":"Viewer","text":"<p>The <code>Viewer</code> provides real-time image/video display with keyboard interaction and video recording capabilities.</p>"},{"location":"guides/visualization/#window-management","title":"Window Management","text":"Method Description <code>is_window_exist_and_closed()</code> Check if user closed window <code>wait_key(ms)</code> Wait for key (timeout in ms) <code>is_open()</code> Check if window is open <p>Always check <code>is_window_exist_and_closed()</code> in your loop.</p>"},{"location":"guides/visualization/#key-handling","title":"Key Handling","text":"<p>Common Keys</p> <ul> <li><code>ESC</code> \u2014 Exit</li> <li><code>Space</code> \u2014 Pause/Resume</li> <li><code>S</code> \u2014 Save screenshot</li> </ul> <p>Example</p> <pre><code>if let Some(key) = viewer.wait_key(30) {\n    match key {\n        Key::Escape =&gt; break,\n        Key::S =&gt; {\n            images[0].save(\"screenshot.png\")?;\n        }\n        _ =&gt; {}\n    }\n}\n</code></pre>"},{"location":"guides/visualization/#video-recording","title":"Video Recording","text":"<p>Requires <code>video</code> feature</p> <p>Add to <code>Cargo.toml</code>: <code>features = [\"video\"]</code></p> <p>Example</p> <pre><code>let mut viewer = Viewer::default()\n    .with_record_output(\"output.mp4\")?;\n\nfor (images, _) in dl {\n    viewer.imshow(&amp;images[0])?;\n    viewer.write_video_frame(&amp;images[0])?;\n\n    if viewer.is_window_exist_and_closed() {\n        break;\n    }\n}\n</code></pre>"},{"location":"guides/visualization/#example","title":"Example","text":"<p>imshow</p> <pre><code>use clap::Parser;\nuse usls::{DataLoader, Source, Viewer};\n\n#[derive(Parser, Debug)]\n#[command(author, version, about, long_about = None)]\nstruct Args {\n    /// Data source.\n    #[arg(long, required = true)]\n    source: Source,\n\n    /// Save frames to video.\n    #[arg(long, default_value = \"false\")]\n    save: bool,\n\n    /// Window scale.\n    #[arg(long, default_value = \"0.8\")]\n    window_scale: f32,\n\n    /// Delay in milliseconds between frames.\n    #[arg(long, default_value = \"1\")]\n    delay: u64,\n\n    /// Num of frames to skip.\n    #[arg(long, default_value = \"0\")]\n    nfv_skip: u64,\n}\n\nfn main() -&gt; anyhow::Result&lt;()&gt; {\n    let args = Args::parse();\n    let dl = DataLoader::new(args.source)?\n        .with_nfv_skip(args.nfv_skip)\n        .stream()?;\n    let mut viewer = Viewer::default().with_window_scale(args.window_scale);\n\n    for images in &amp;dl {\n        // Check if the window is closed and exit if so.\n        if viewer.is_window_exist_and_closed() {\n            break;\n        }\n\n        // Display the current image.\n        viewer.imshow(&amp;images[0])?;\n\n        // Wait for a key press or timeout, and exit on Escape.\n        if let Some(key) = viewer.wait_key(args.delay) {\n            if key == usls::Key::Escape {\n                break;\n            }\n        }\n\n        // Save the current frame to video if requested.\n        // Note: For multiple videos, frames will be saved to separate files.\n        if args.save {\n            viewer.write_video_frame(&amp;images[0])?;\n        }\n    }\n\n    Ok(())\n}\n</code></pre>"},{"location":"model-zoo/background-removal/","title":"\u2728 Background Removal","text":"<p>Status: \u2705 Supported | \u2753 Unknown | \u274c Not Supported For Now</p> Model Task / Description Demo Dynamic Batch TensorRT FP32 FP16 Q8 Q4f16 BNB4 RMBG Image SegmentationBackground Removal demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705 BEN2 Image SegmentationBackground Removal demo \u2705 \u2753 \u2705 \u2705 \u274c \u274c \u274c"},{"location":"model-zoo/classification/","title":"\ud83c\udff7\ufe0f Image Classification &amp; Tagging","text":"<p>Status: \u2705 Supported | \u2753 Unknown | \u274c Not Supported For Now</p> Model Task / Description Demo Dynamic Batch TensorRT FP32 FP16 Q8 Q4f16 BNB4 BEiT Image Classification demo \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c ConvNeXt Image Classification demo \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c FastViT Image Classification demo \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c MobileOne Image Classification demo \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c DeiT Image Classification demo \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c RAM Image Tagging demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705 RAM++ Image Tagging demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705"},{"location":"model-zoo/depth/","title":"\ud83d\udcd0 Depth Estimation","text":"<p>Status: \u2705 Supported | \u2753 Unknown | \u274c Not Supported For Now</p> Model Task / Description Demo Dynamic Batch TensorRT FP32 FP16 Q8 Q4f16 BNB4 DepthAnything v1 Monocular Depth Estimation demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705 DepthAnything v2 Monocular Depth Estimation demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705 DepthPro Monocular Depth Estimation demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705 Depth-Anything-3 MonocularMetricMulti-View demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705"},{"location":"model-zoo/detection/","title":"\ud83c\udfaf Object Detection","text":"<p>Status: \u2705 Supported | \u2753 Unknown | \u274c Not Supported For Now</p> Model Task / Description Demo Dynamic Batch TensorRT FP32 FP16 Q8 Q4f16 BNB4 RT-DETRv1 Object Detection demo \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 RT-DETRv2 Object Detection demo \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 RT-DETRv4 Object Detection demo \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 RF-DETR Object Detection demo \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 PP-PicoDet Object Detection demo \u274c \u2753 \u2705 \u274c \u274c \u274c \u274c D-FINE Object Detection demo \u2705 \u2753 \u2705 \u274c \u274c \u274c \u274c DEIM Object Detection demo \u2705 \u2753 \u2705 \u274c \u274c \u274c \u274c DEIMv2 Object Detection demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705"},{"location":"model-zoo/embedding/","title":"\ud83e\uddec Embedding Models","text":"<p>Status: \u2705 Supported | \u2753 Unknown | \u274c Not Supported For Now</p> Model Task / Description Demo Dynamic Batch TensorRT FP32 FP16 Q8 Q4f16 BNB4 CLIP Vision-Language Embedding demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705 jina-clip-v1 Vision-Language Embedding demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705 jina-clip-v2 Vision-Language Embedding demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705 mobileclip Vision-Language Embedding demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705 DINOv2 Vision Embedding demo \u2705 \u2753 \u2705 \u274c \u274c \u274c \u274c DINOv3 Vision Embedding demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705"},{"location":"model-zoo/gaze/","title":"\ud83d\udc40 Gaze Estimation","text":"<p>Status: \u2705 Supported | \u2753 Unknown | \u274c Not Supported For Now</p> Model Task / Description Demo Dynamic Batch TensorRT FP32 FP16 Q8 Q4f16 BNB4 MobileGaze Eye Gaze Estimation demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705"},{"location":"model-zoo/matting/","title":"\u2702\ufe0f Image Matting &amp; Portrait Segmentation","text":"<p>Status: \u2705 Supported | \u2753 Unknown | \u274c Not Supported For Now</p> Model Task / Description Demo Dynamic Batch TensorRT FP32 FP16 Q8 Q4f16 BNB4 MODNet Image Matting demo \u2705 \u2753 \u2705 \u2705 \u2705 \u274c \u274c MediaPipe Selfie Image Segmentation demo \u2705 \u2753 \u2705 \u2705 \u2705 \u274c \u274c BiRefNet - Portrait Portrait Background Removal demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705 BiRefNet - Matting Portrait Matting &amp; Background Removal demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705 BiRefNet - HR Matting High-Resolution Portrait Matting demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705 BiRefNet - General General Purpose Segmentation demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705 BiRefNet - HR General High-Resolution General Segmentation demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705 BiRefNet - Lite General Lightweight General Segmentation (2K) demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705 BiRefNet - General Tiny Lightweight General Segmentation with Swin-V1-Tiny demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705"},{"location":"model-zoo/ocr/","title":"\ud83d\udd0d OCR &amp; Document Understanding","text":"<p>Status: \u2705 Supported | \u2753 Unknown | \u274c Not Supported For Now</p> Model Task / Description Demo Dynamic Batch TensorRT FP32 FP16 Q8 Q4f16 BNB4 DB Text Detection demo \u2705 \u2753 \u2705 \u2705 \u274c \u274c \u274c FAST Text Detection demo \u2705 \u2753 \u2705 \u2705 \u274c \u274c \u274c LinkNet Text Detection demo \u2705 \u2753 \u2705 \u2705 \u274c \u274c \u274c SVTR Text Recognition demo \u2705 \u2753 \u2705 \u2705 \u274c \u274c \u274c TrOCR Text Recognition demo \u2705 \u2753 \u2705 \u2705 \u274c \u274c \u274c SLANet Table Recognition demo \u2705 \u2753 \u2705 \u2705 \u274c \u274c \u274c DocLayout-YOLO Object Detection demo \u2705 \u2753 \u2705 \u2705 \u2705 \u274c \u274c PP-DocLayout-v1-Plus-L Object Detection demo \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 PP-DocLayout-v2 Object Detection demo \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705"},{"location":"model-zoo/open-set/","title":"\ud83d\uddfa\ufe0f Open-Set Detection &amp; Segmentation","text":"<p>Status: \u2705 Supported | \u2753 Unknown | \u274c Not Supported For Now</p> Model Task / Description Demo Dynamic Batch TensorRT FP32 FP16 Q8 Q4f16 BNB4 GroundingDINO Open-Set Detection With Language demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705 MM-GDINO Open-Set Detection With Language demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705 LLMDet Open-Set Detection With Language demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705 OWLv2 Open-Set Object Detection demo \u2705 \u2753 \u2705 \u2705 \u274c \u274c \u274c YOLO-World Open-Set Detection With Language demo \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 YOLOE-Prompt-Based Open-Set Detection And Segmentation demo \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 YOLOE-26-Prompt-Based Open-Set Detection And Segmentation demo \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 SAM3-Image Open-Set Detection And Segmentation demo \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705"},{"location":"model-zoo/others/","title":"\ud83c\udf0c Others","text":"<p>Status: \u2705 Supported | \u2753 Unknown | \u274c Not Supported For Now</p> Model Task / Description Demo Dynamic Batch TensorRT FP32 FP16 Q8 Q4f16 BNB4 Sapiens Foundation for Human Vision Models demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705 YOLOPv2 Panoptic Driving demo \u2705 \u2753 \u2705 \u274c \u274c \u274c \u274c"},{"location":"model-zoo/overview/","title":"Model Zoo","text":"<p>ONNX Models</p> <p>\ud83d\udd0d All ONNX models are available from the ONNX Models Repository</p>"},{"location":"model-zoo/overview/#categories","title":"Categories","text":"<ul> <li> <p> YOLO Series</p> <p>YOLOv5/6/7/8/9/10/11/12/13, YOLO26</p> </li> <li> <p> Classification &amp; Tagging</p> <p>Image classification and tagging models</p> </li> <li> <p> Object Detection</p> <p>DETR series, PicoDet, D-FINE, DEIM</p> </li> <li> <p> Image Segmentation</p> <p>SAM series, FastSAM, YOLOE, BiRefNet</p> </li> <li> <p> Background Removal</p> <p>RMBG, BEN2</p> </li> <li> <p> Gaze Estimation</p> <p>MobileGaze</p> </li> <li> <p> Image Matting</p> <p>MODNet, MediaPipe Selfie, BiRefNet variants</p> </li> <li> <p> Open-Set Detection</p> <p>GroundingDINO, MM-GDINO, LLMDet, OWLv2, YOLO-World</p> </li> <li> <p> Multi-Object Tracking</p> <p>ByteTrack</p> </li> <li> <p> Super Resolution</p> <p>Swin2SR, APISR</p> </li> <li> <p> Pose Estimation</p> <p>RTMPose, DWPose, RTMW, RTMO</p> </li> <li> <p> OCR &amp; Documents</p> <p>Text detection/recognition, table recognition, document layout</p> </li> <li> <p> Vision-Language Models</p> <p>BLIP, Florence2, Moondream2, SmolVLM, FastVLM</p> </li> <li> <p> Embedding Models</p> <p>CLIP, jina-clip, MobileCLIP, DINOv2/v3</p> </li> <li> <p> Depth Estimation</p> <p>DepthAnything, DepthPro</p> </li> <li> <p> Others</p> <p>Sapiens, YOLOPv2</p> </li> </ul>"},{"location":"model-zoo/overview/#status-legend","title":"Status Legend","text":"Symbol Meaning \u2705 Tested &amp; verified \u2753 Likely works, not verified \u274c Not supported <p>Missing a model?</p> <p>Open a feature request.</p>"},{"location":"model-zoo/pose/","title":"\ud83e\udd38 Pose Estimation","text":"<p>Status: \u2705 Supported | \u2753 Unknown | \u274c Not Supported For Now</p> Model Task / Description Demo Dynamic Batch TensorRT FP32 FP16 Q8 Q4f16 BNB4 RTMPose Keypoint Detection demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705 DWPose Keypoint Detection demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705 RTMW Keypoint Detection demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705 RTMO Keypoint Detection demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u274c"},{"location":"model-zoo/segmentation/","title":"\ud83c\udfa8 Image Segmentation","text":"<p>Status: \u2705 Supported | \u2753 Unknown | \u274c Not Supported For Now</p> Model Task / Description Demo Dynamic Batch TensorRT FP32 FP16 Q8 Q4f16 BNB4 SAM Segment Anything demo \u2705 \u2753 \u2705 \u274c \u274c \u274c \u274c SAM-HQ Segment Anything demo \u2705 \u2753 \u2705 \u274c \u274c \u274c \u274c MobileSAM Segment Anything demo \u2705 \u2753 \u2705 \u274c \u274c \u274c \u274c EdgeSAM Segment Anything demo \u2705 \u2753 \u2705 \u274c \u274c \u274c \u274c YOLOE-v8/11-Prompt-Free Open-Set Detection And Segmentation demo \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 YOLOE-26-Prompt-Free Open-Set Detection And Segmentation demo \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 FastSAM Instance Segmentation demo \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 SAM2 Segment Anything demo \u2705 \u2753 \u2705 \u274c \u274c \u274c \u274c SAM3-Tracker Segment Anything demo \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 BiRefNet - COD Camouflaged Object Detection demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705 BiRefNet - DIS Dichotomous Image Segmentation demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705 BiRefNet - HRSOD High-Resolution Salient Object Detection demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705 BiRefNet - Massive Multi-Dataset Robust Segmentation demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705"},{"location":"model-zoo/super-resolution/","title":"\ud83d\udc8e Image Super-Resolution","text":"<p>Status: \u2705 Supported | \u2753 Unknown | \u274c Not Supported For Now</p> Model Task / Description Demo Dynamic Batch TensorRT FP32 FP16 Q8 Q4f16 BNB4 Swin2SR Image Restoration demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705 APISR Anime Super-Resolution demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705"},{"location":"model-zoo/tracking/","title":"\ud83c\udfc3 Multi-Object Tracking","text":"<p>Status: \u2705 Supported | \u2753 Unknown | \u274c Not Supported For Now</p> Model Task / Description Demo Dynamic Batch TensorRT FP32 FP16 Q8 Q4f16 BNB4 ByteTrack Multi-Object Tracking demo \u274c \u274c \u274c \u274c \u274c \u274c \u274c"},{"location":"model-zoo/vlm/","title":"\ud83e\udde9 Vision-Language Models (VLM)","text":"<p>Status: \u2705 Supported | \u2753 Unknown | \u274c Not Supported For Now</p> Model Task / Description Demo Dynamic Batch TensorRT FP32 FP16 Q8 Q4f16 BNB4 BLIP Image Captioning demo \u2705 \u2753 \u2705 \u2753 \u274c \u274c \u274c Florence2 A Variety of Vision Tasks demo \u2705 \u2753 \u2705 \u2705 \u274c \u274c \u274c Moondream2 Open-Set Object DetectionOpen-Set Keypoints DetectionImage CaptioningVisual Question Answering demo \u2705 \u2753 \u274c \u274c \u2705 \u2705 \u274c SmolVLM Visual Question Answering demo \u2705 \u2753 \u2705 \u2753 \u2753 \u2753 \u2753 SmolVLM2 Visual Question Answering demo \u2705 \u2753 \u2705 \u2753 \u2753 \u2753 \u2753 FastVLM Vision Language Models demo \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 \u2705"},{"location":"model-zoo/yolo/","title":"\ud83d\udd25 YOLO Series","text":"<p>Status: \u2705 Supported | \u2753 Unknown | \u274c Not Supported For Now</p> Model Task / Description Demo Dynamic Batch TensorRT FP32 FP16 Q8 Q4f16 BNB4 YOLOv5 Image ClassificationObject DetectionInstance Segmentation demo \u2705 \u2705 \u2705 \u2705 \u2705 \u274c \u274c YOLOv6 Object Detection demo \u2705 \u2705 \u2705 \u2705 \u2705 \u274c \u274c YOLOv7 Object Detection demo \u2705 \u2705 \u2705 \u2705 \u2705 \u274c \u274c YOLOv8 Object DetectionInstance SegmentationImage ClassificationOriented Object DetectionKeypoint Detection demo \u2705 \u2705 \u2705 \u2705 \u2705 \u274c \u274c YOLO11 Object DetectionInstance SegmentationImage ClassificationOriented Object DetectionKeypoint Detection demo \u2705 \u2705 \u2705 \u2705 \u2705 \u274c \u274c YOLOv9 Object Detection demo \u2705 \u2705 \u2705 \u2705 \u2705 \u274c \u274c YOLOv10 Object Detection demo \u2705 \u2705 \u2705 \u2705 \u2705 \u274c \u274c YOLOv12 Image ClassificationObject DetectionInstance Segmentation demo \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 YOLOv13 Object Detection demo \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 YOLO26 Object DetectionInstance SegmentationImage ClassificationOriented Object DetectionKeypoint Detection demo \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705"}]}