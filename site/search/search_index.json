{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83d\ude80 usls","text":"<p>usls is a cross-platform Rust library powered by ONNX Runtime for efficient inference of SOTA vision and vision-language models (typically under 1B parameters).</p>"},{"location":"#highlights","title":"\u26a1 Highlights","text":"<ul> <li>\u26a1 High Performance: Multi-threading, SIMD, and CUDA-accelerated processing</li> <li>\ud83c\udf10 Cross-Platform: Linux, macOS, Windows with ONNX Runtime execution providers (CUDA, TensorRT, CoreML, OpenVINO, DirectML, etc.)</li> <li>\ud83c\udfd7\ufe0f Unified API: Single <code>Model</code> trait inference with <code>run()</code>/<code>forward()</code>/<code>encode_images()</code>/<code>encode_texts()</code> and unified <code>Y</code> output</li> <li>\ud83d\udce5 Auto-Management: Automatic model download (HuggingFace/GitHub), caching and path resolution</li> <li>\ud83d\udce6 Multiple Inputs: Image, directory, video, webcam, stream and combinations</li> <li>\ud83c\udfaf Precision Support: FP32, FP16, INT8, UINT8, Q4, Q4F16, BNB4, and more</li> <li>\ud83d\udee0\ufe0f Full-Stack Suite: <code>DataLoader</code>, <code>Annotator</code>, and <code>Viewer</code> for complete workflows</li> <li>\ud83c\udf31 Model Ecosystem: 50+ SOTA vision and VLM models</li> </ul>"},{"location":"#acknowledgments","title":"\ud83e\udd1d Acknowledgments","text":"<p>Built on top of ort (ONNX Runtime for Rust). Special thanks to all open-source contributors and maintainers of the underlying models.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>TODO</p> <p>Questions?</p> <p>If you're unsure where to start, feel free to open a discussion or join our community.</p>"},{"location":"faq/","title":"FAQ","text":"<p>TODO</p> <p>Still have questions?</p> <p>If you can't find what you're looking for, feel free to open a GitHub Issue.</p>"},{"location":"cargo-features/overview/","title":"Cargo Features","text":"<p>usls is highly modular. Use feature flags to include only the models and hardware support you need, keeping your binary small and compilation fast.</p> <p>Features in italics are enabled by default.</p>"},{"location":"cargo-features/overview/#core-utilities","title":"Core &amp; Utilities","text":"<ul> <li><code>ort-download-binaries</code>: Automatically download prebuilt ONNX Runtime binaries from pyke.</li> <li><code>ort-load-dynamic</code>: Manually link ONNX Runtime. Useful for custom builds or unsupported platforms. See Linking Guide for more details.</li> <li><code>viewer</code>: Real-time image/video visualization (similar to OpenCV <code>imshow</code>). Empowered by minifb.</li> <li><code>video</code>: Video I/O support for reading and writing video streams. Empowered by video-rs.</li> <li><code>hf-hub</code>: Download model files from Hugging Face Hub.</li> <li><code>annotator</code>: Annotation utilities for drawing bounding boxes, keypoints, and masks on images.</li> </ul>"},{"location":"cargo-features/overview/#image-formats","title":"Image Formats","text":"<p>Additional image format support (optional for faster compilation):</p> <ul> <li><code>image-all-formats</code>: Enable all additional image formats.</li> <li><code>image-gif</code>, <code>image-bmp</code>, <code>image-ico</code>, <code>image-avif</code>, <code>image-tiff</code>, <code>image-dds</code>, <code>image-exr</code>, <code>image-ff</code>, <code>image-hdr</code>, <code>image-pnm</code>, <code>image-qoi</code>, `image-tga: Individual image format support.</li> </ul>"},{"location":"cargo-features/overview/#model-categories","title":"Model Categories","text":"<ul> <li><code>vision</code>: Core vision models (Detection, Segmentation, Classification, Pose, etc.).</li> <li><code>vlm</code>: Vision-Language Models (CLIP, BLIP, Florence2, etc.).</li> <li><code>mot</code>: Multi-Object Tracking utilities.</li> <li><code>all-models</code>: Enable all model categories.</li> </ul>"},{"location":"cargo-features/overview/#execution-providers","title":"Execution Providers","text":"<p>Hardware acceleration for inference. Enable the one matching your hardware:</p> <ul> <li><code>cuda</code>: NVIDIA CUDA execution provider (pure model inference acceleration).</li> <li><code>tensorrt</code>: NVIDIA TensorRT execution provider (pure model inference acceleration).</li> <li><code>nvrtx</code>: NVIDIA NvTensorRT-RTX execution provider (pure model inference acceleration).</li> <li><code>cuda-full</code>: <code>cuda</code> + <code>cuda-runtime-build</code> (Model + Image Preprocessing acceleration).</li> <li><code>tensorrt-full</code>: <code>tensorrt</code> + <code>cuda-runtime-build</code> (Model + Image Preprocessing acceleration).</li> <li><code>nvrtx-full</code>: <code>nvrtx</code> + <code>cuda-runtime-build</code> (Model + Image Preprocessing acceleration).</li> <li><code>coreml</code>: Apple Silicon (macOS/iOS).</li> <li><code>openvino</code>: Intel CPU/GPU/VPU.</li> <li><code>onednn</code>: Intel Deep Neural Network Library.</li> <li><code>directml</code>: DirectML (Windows).</li> <li><code>webgpu</code>: WebGPU (Web/Chrome).</li> <li><code>rocm</code>: AMD GPU acceleration.</li> <li><code>cann</code>: Huawei Ascend NPU.</li> <li><code>rknpu</code>: Rockchip NPU.</li> <li><code>xnnpack</code>: Mobile CPU optimization.</li> <li><code>acl</code>: Arm Compute Library.</li> <li><code>armnn</code>: Arm Neural Network SDK.</li> <li><code>azure</code>: Azure ML execution provider.</li> <li><code>migraphx</code>: AMD MIGraphX.</li> <li><code>nnapi</code>: Android Neural Networks API.</li> <li><code>qnn</code>: Qualcomm SNPE.</li> <li><code>tvm</code>: Apache TVM.</li> <li><code>vitis</code>: Xilinx Vitis AI.</li> </ul>"},{"location":"cargo-features/overview/#cuda-support","title":"CUDA Support","text":"<p>NVIDIA GPU acceleration with CUDA image processing kernels (requires <code>cudarc</code>):</p> <ul> <li><code>cuda-full</code>: Uses <code>cuda-version-from-build-system</code> (auto-detects via <code>nvcc</code>).</li> <li><code>cuda-11040</code>, <code>cuda-11050</code>, <code>cuda-11060</code>, <code>cuda-11070</code>, <code>cuda-11080</code>: CUDA 11.x versions (Model + Preprocess).</li> <li><code>cuda-12000</code>, <code>cuda-12010</code>, <code>cuda-12020</code>, <code>cuda-12030</code>, <code>cuda-12040</code>, <code>cuda-12050</code>, <code>cuda-12060</code>, <code>cuda-12080</code>, <code>cuda-12090</code>: CUDA 12.x versions (Model + Preprocess).</li> <li><code>cuda-13000</code>, <code>cuda-13010</code>: CUDA 13.x versions (Model + Preprocess).</li> </ul>"},{"location":"cargo-features/overview/#tensorrt-support","title":"TensorRT Support","text":"<p>NVIDIA TensorRT execution provider with CUDA runtime libraries:</p> <ul> <li><code>tensorrt-full</code>: Uses <code>cuda-version-from-build-system</code> (auto-detects via <code>nvcc</code>).</li> <li><code>tensorrt-cuda-11040</code>, <code>tensorrt-cuda-11050</code>, <code>tensorrt-cuda-11060</code>, <code>tensorrt-cuda-11070</code>, <code>tensorrt-cuda-11080</code>: TensorRT + CUDA 11.x runtime.</li> <li><code>tensorrt-cuda-12000</code>, <code>tensorrt-cuda-12010</code>, <code>tensorrt-cuda-12020</code>, <code>tensorrt-cuda-12030</code>, <code>tensorrt-cuda-12040</code>, <code>tensorrt-cuda-12050</code>, <code>tensorrt-cuda-12060</code>, <code>tensorrt-cuda-12080</code>, <code>tensorrt-cuda-12090</code>: TensorRT + CUDA 12.x runtime.</li> <li><code>tensorrt-cuda-13000</code>, <code>tensorrt-cuda-13010</code>: TensorRT + CUDA 13.x runtime.</li> </ul> <p>Note: <code>tensorrt-cuda-*</code> features enable TensorRT execution provider with CUDA runtime libraries for image processing. The \"cuda\" in the name refers to <code>cudarc</code> dependency.</p>"},{"location":"cargo-features/overview/#nvrtx-support","title":"NVRTX Support","text":"<p>NVIDIA NvTensorRT-RTX execution provider with CUDA runtime libraries:</p> <ul> <li><code>nvrtx-full</code>: Uses <code>cuda-version-from-build-system</code> (auto-detects via <code>nvcc</code>). </li> <li><code>nvrtx-cuda-11040</code>, <code>nvrtx-cuda-11050</code>, <code>nvrtx-cuda-11060</code>, <code>nvrtx-cuda-11070</code>, <code>nvrtx-cuda-11080</code>: NVRTX + CUDA 11.x runtime.</li> <li><code>nvrtx-cuda-12000</code>, <code>nvrtx-cuda-12010</code>, <code>nvrtx-cuda-12020</code>, <code>nvrtx-cuda-12030</code>, <code>nvrtx-cuda-12040</code>, <code>nvrtx-cuda-12050</code>, <code>nvrtx-cuda-12060</code>, <code>nvrtx-cuda-12080</code>, <code>nvrtx-cuda-12090</code>: NVRTX + CUDA 12.x runtime.</li> <li><code>nvrtx-cuda-13000</code>, <code>nvrtx-cuda-13010</code>: NVRTX + CUDA 13.x runtime.</li> </ul> <p>Note: <code>nvrtx-cuda-*</code> features enable NVRTX execution provider with CUDA runtime libraries for image processing. The \"cuda\" in the name refers to <code>cudarc</code> dependency.</p>"},{"location":"cargo-features/overview/#device-combination-guide","title":"\ud83d\ude80 Device Combination Guide","text":"Scenario Model Device (<code>--device</code>) Processor Device (<code>--processor-device</code>) Required Features (<code>-F</code>) CPU Only <code>cpu</code> <code>cpu</code> <code>vision</code> (default) GPU Inference (Slow Preprocess) <code>cuda</code> <code>cpu</code> <code>cuda</code> GPU Inference (Fast Preprocess) <code>cuda</code> <code>cuda</code> <code>cuda-full</code> or <code>cuda-120xxx</code> TensorRT (Slow Preprocess) <code>tensorrt</code> <code>cpu</code> <code>tensorrt</code> TensorRT (Fast Preprocess) <code>tensorrt</code> <code>cuda</code> <code>tensorrt-full</code> or <code>tensorrt-cuda-120xxx</code> <p>\u26a0\ufe0f In multi-GPU environments (e.g., <code>cuda:0</code>, <code>cuda:1</code>), you MUST ensure that both <code>--device</code> and <code>--processor-device</code> use the SAME GPU ID. </p>"},{"location":"cargo-features/overview/#common-pitfalls","title":"Common Pitfalls","text":"<pre><code># \u274c Don't mix multiple CUDA versions\nfeatures = [\"cuda-12040\", \"cuda-11080\"]\n\n# \u2705 Use one execution provider\nfeatures = [\"tensorrt-full\"]\n\n# \u2705 Use two execution provider: cuda EP + tensorrt EP + cuda image processing\nfeatures = [\"cuda-full\", \"tensorrt\"]\nfeatures = [\"cuda\", \"tensorrt-full\"]\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>To use usls in your project, add it to your <code>Cargo.toml</code>.</p> Crates.ioGitHub (Recommended) <pre><code>[dependencies]\nusls = { version = \"latest-version\", features = [ \"cuda\" ] }\n</code></pre> <pre><code>[dependencies]\nusls = { git = \"https://github.com/jamjamjon/usls\", branch = \"main\" }\n</code></pre>"},{"location":"getting-started/integration/","title":"Integration Workflow","text":"<p><code>usls</code> implements a clean, modular pipeline from data ingestion to results visualization.</p>"},{"location":"getting-started/integration/#the-4-step-pipeline","title":"The 4-Step Pipeline","text":"<ol> <li>Configure Model: Select a pre-configured model (e.g., <code>Config::rfdetr_nano()</code>), customize settings, and commit the configuration.</li> <li>Load Data: Setup a <code>DataLoader</code> to handle your input sources (images, videos, etc.).</li> <li>Inference: Iterate through the <code>DataLoader</code> and pass data to <code>model.run()</code> or <code>model.forward()</code>.</li> <li>Extract Results: Access detections, masks, or embeddings from the unified <code>Y</code> output.</li> <li>Annotate (Optional): Use the <code>Annotator</code> to draw results back onto the original images.</li> <li>Visualize (Optional): Use the <code>Viewer</code> for real-time display or video recording.</li> </ol>"},{"location":"getting-started/integration/#basic-usage-example","title":"Basic Usage Example","text":"<pre><code>use usls::*;\n\nfn main() -&gt; anyhow::Result&lt;()&gt; {\n    // 1. Configure &amp; Build Model\n    let config = Config::rfdetr_nano()\n        .with_model_device(Device::Cuda(0))\n        .commit()?;\n    let mut model = RFDETR::new(config)?;\n\n    // 2. Setup DataLoader\n    let dl = DataLoader::new(\"image.jpg\")?\n        .with_batch(model.batch())\n        .stream()?;\n\n    // optional: Annotate\n    let annotator = Annotator::default();\n\n    // optional: Viewer\n    let mut viewer = Viewer::default();\n\n    // 3. Run Inference\n    for xs in dl {\n        let ys = model.run(&amp;xs)?;\n        for (x, y) in xs.iter().zip(ys.iter()) {\n            // 4. Access results\n            for hb in y.hbbs() {\n                println!(\"{}\", hb);\n            }\n\n            // optional: Check if the window is closed and exit if so.\n            if viewer.is_window_exist_and_closed() {\n                break;\n            }\n\n            // optional: Annotate\n            let image_annotated = annotator.annotate(x, y)?;\n\n            // optional: Display the current image.\n            viewer.imshow(&amp;image_annotated)?;\n\n            // optional: Wait for a key press or timeout, and exit on Escape.\n            if let Some(key) = viewer.wait_key(10) {\n                if key == usls::Key::Escape {\n                    break;\n                }\n            }\n\n            // optional: Save the annotated image.\n            image_annotated.save(\"output.jpg\")?;\n        }\n    }\n    Ok(())\n}\n</code></pre>"},{"location":"getting-started/overview/","title":"Getting Started Overview","text":"<p>usls is a cross-platform Rust library powered by ONNX Runtime for efficient inference of SOTA vision and vision-language models (typically under 1B parameters).</p>"},{"location":"getting-started/overview/#highlights","title":"\ud83c\udf1f Highlights","text":"<ul> <li>\u26a1 High Performance: Multi-threading, SIMD, and CUDA-accelerated processing</li> <li>\ud83c\udf10 Cross-Platform: Linux, macOS, Windows with ONNX Runtime execution providers (CUDA, TensorRT, CoreML, OpenVINO, DirectML, etc.)</li> <li>\ud83c\udfd7\ufe0f Unified API: Single <code>Model</code> trait inference with <code>run()</code>/<code>forward()</code>/<code>encode_images()</code>/<code>encode_texts()</code> and unified <code>Y</code> output</li> <li>\ud83d\udce5 Auto-Management: Automatic model download (HuggingFace/GitHub), caching and path resolution</li> <li>\ud83d\udce6 Multiple Inputs: Image, directory, video, webcam, stream and combinations</li> <li>\ud83c\udfaf Precision Support: FP32, FP16, INT8, UINT8, Q4, Q4F16, BNB4, and more</li> <li>\ud83d\udee0\ufe0f Full-Stack Suite: <code>DataLoader</code>, <code>Annotator</code>, and <code>Viewer</code> for complete workflows</li> <li>\ud83c\udf31 Model Ecosystem: 50+ SOTA vision and VLM models</li> </ul>"},{"location":"getting-started/run_demo/","title":"Run Demo","text":"<p>Let's run the YOLO-Series demo to explore models with different tasks, precision and execution providers:</p> <ul> <li>Tasks: <code>detect</code>, <code>segment</code>, <code>pose</code>, <code>classify</code>, <code>obb</code></li> <li>Versions: <code>5</code>, <code>6</code>, <code>7</code>, <code>8</code>, <code>9</code>, <code>10</code>, <code>11</code>, <code>12</code>, <code>13</code>, <code>26</code></li> <li>Scales: <code>n</code>, <code>s</code>, <code>m</code>, <code>l</code>, <code>x</code></li> <li>Precision (DType): <code>fp32</code>, <code>fp16</code>, <code>q8</code>, <code>q4</code>, <code>q4f16</code>, <code>bnb4</code></li> <li>Devices: <code>cpu</code>, <code>cuda:0</code>, <code>tensorrt:0</code>, <code>coreml</code>, <code>openvino:CPU</code></li> </ul>"},{"location":"getting-started/run_demo/#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>First, clone the repository and navigate to the project root:</p> <pre><code>git clone https://github.com/jamjamjon/usls.git\ncd usls\n</code></pre> <p>Then, choose the command that matches your hardware:</p> CPU (Default)NVIDIA GPU (CUDA)NVIDIA GPU (TensorRT)Apple Silicon (CoreML) <pre><code># Object detection with YOLO26n (FP16)\ncargo run -r --example yolo -- --task detect --ver 26 --scale n --dtype fp16\n</code></pre> <pre><code># Requires \"cuda-full\" feature\ncargo run -r -F cuda-full --example yolo -- --task segment --ver 11 --scale m --device cuda:0 --processor-device cuda:0\n</code></pre> <pre><code># Requires \"tensorrt-full\" feature\ncargo run -r -F tensorrt-full --example yolo -- --device tensorrt:0 --processor-device cuda:0\n</code></pre> <pre><code># Requires \"coreml\" feature\ncargo run -r -F coreml --example yolo -- --device coreml\n</code></pre> <p>For a full list of options, run: <pre><code>cargo run -r --example yolo -- --help\n</code></pre></p>"},{"location":"getting-started/run_demo/#performance-reference","title":"\ud83d\udcca Performance Reference","text":"<p>Environment: NVIDIA RTX 3060Ti (CUDA 12.8) / Intel i5-12400F Setup: YOLO26n, 640x640 resolution, COCO2017 val set (5,000 images)</p> EP ImageProcessor DType Batch Preprocess Inference Postprocess Total TensorRT CUDA FP16 1 ~233\u00b5s ~1.3ms ~14\u00b5s ~1.55ms TensorRT-RTX CUDA FP32 1 ~233\u00b5s ~2.0ms ~10\u00b5s ~2.24ms TensorRT-RTX CUDA FP16 1 \u2753 \u2753 \u2753 \u2753 CUDA CUDA FP32 1 ~233\u00b5s ~5.0ms ~17\u00b5s ~5.25ms CUDA CUDA FP16 1 ~233\u00b5s ~3.6ms ~17\u00b5s ~3.85ms CUDA CPU FP32 1 ~800\u00b5s ~6.5ms ~14\u00b5s ~7.31ms CUDA CPU FP16 1 ~800\u00b5s ~5.0ms ~14\u00b5s ~5.81ms CPU CPU FP32 1 ~970\u00b5s ~20.5ms ~14\u00b5s ~21.48ms CPU CPU FP16 1 ~970\u00b5s ~25.0ms ~14\u00b5s ~25.98ms TensorRT CUDA FP16 8 ~1.2ms ~6.0ms ~55\u00b5s ~7.26ms TensorRT CPU FP16 8 ~18.0ms ~25.5ms ~55\u00b5s ~43.56ms <p>Multi-Batch Performance</p> <p>When using a larger batch size (e.g., batch 8), CUDA Image processor significantly improves throughput on GPUs.</p>"},{"location":"guides/annotator/","title":"Annotator","text":"<p>The <code>Annotator</code> is a powerful utility for visualizing model results by drawing bounding boxes, masks, keypoints, and other metadata directly onto images.</p>"},{"location":"guides/annotator/#core-features","title":"Core Features","text":"<ul> <li>Standardized Visualization: Works seamlessly with the unified <code>Y</code> output.</li> <li>Customizable Styles: Configure colors, thickness, transparency, and fonts per shape type.</li> <li>Multiple Shape Support:<ul> <li>HBB: Horizontal Bounding Boxes with labels and confidence.</li> <li>OBB: Oriented Bounding Boxes.</li> <li>Masks: Semi-transparent overlays for segmentation.</li> <li>Polygons: Contour and polygon drawing.</li> <li>Keypoints: Skeleton drawing for pose estimation.</li> <li>Probs: Bar charts or text overlays for classification probabilities.</li> </ul> </li> </ul>"},{"location":"guides/annotator/#basic-usage","title":"Basic Usage","text":"<p>TODO</p>"},{"location":"guides/annotator/#customizing-styles","title":"Customizing Styles","text":"<p>TODO</p>"},{"location":"guides/config/","title":"Config System","text":"<p>The <code>Config</code> system in usls follows a builder pattern with a strict naming convention, making it easy to discover and use APIs.</p>"},{"location":"guides/config/#api-naming-convention","title":"API Naming Convention","text":"<p>API naming follows 2 predictable patterns:</p> <ol> <li>Per-Module: <ul> <li><code>with_&lt;module_name&gt;_&lt;field_name&gt;(&lt;value&gt;)</code></li> <li><code>with_module_&lt;field_name&gt;(&lt;module_name&gt;, &lt;value&gt;)</code></li> </ul> </li> <li>Global: <ul> <li><code>with_&lt;field_name&gt;_all(&lt;value&gt;)</code></li> </ul> </li> </ol>"},{"location":"guides/config/#common-modules","title":"Common Modules","text":"<ul> <li><code>model</code>: Primary model module.</li> <li><code>visual</code>: Visual encoder/decoder in VLMs.</li> <li><code>textual</code>: Textual encoder/decoder in VLMs.</li> </ul>"},{"location":"guides/config/#example-usage","title":"Example Usage","text":"<pre><code>let config = Config::yolo()\n    .with_model_device(Device::Cuda(0))    // Set device for model\n    .with_model_dtype(DType::Fp16)         // Set precision for model\n    .with_batch_all(8)                     // Set batch size (global)\n    .commit()?;\n</code></pre>"},{"location":"guides/config/#global-vs-per-module-config","title":"Global vs. Per-Module Config","text":"<p>You can apply settings to all modules at once or target specific ones:</p> <pre><code>// Apply to all modules\nconfig.with_device_all(Device::Cuda(0));\n\n// Target specific module\nconfig.with_model_device(Device::Cuda(0));\nconfig.with_module_device(Module::Model, Device::Cuda(0));\n</code></pre>"},{"location":"guides/config/#dynamic-shapes-tensorrt","title":"Dynamic Shapes (TensorRT)","text":"<p>Configure dynamic axes similar to <code>trtexec</code>:</p> <pre><code>config.with_model_ixx(0, 0, (1, 1, 8)); // axis 0 (batch): min=1, opt=1, max=8\n</code></pre>"},{"location":"guides/dataloader/","title":"Data Loading","text":"<p>usls provides a flexible and efficient <code>DataLoader</code> system designed for high-performance inference. It supports various input sources, automatic batching, and parallel processing.</p>"},{"location":"guides/dataloader/#supported-sources","title":"\ud83d\udce5 Supported Sources","text":"<p>The <code>DataLoader</code> can ingest data from multiple formats:</p> <ul> <li>Images: Single files (<code>.jpg</code>, <code>.png</code>), directories, or lists of paths.</li> <li>Videos: Local video files or RTSP/HTTP streams.</li> <li>Hardware: Webcams and other camera devices.</li> <li>Memory: Raw bytes or pre-loaded images.</li> </ul>"},{"location":"guides/dataloader/#basic-usage","title":"\ud83d\udd04 Basic Usage","text":""},{"location":"guides/dataloader/#read-into-image-or-vec","title":"Read into Image or Vec <pre><code>use usls::DataLoader;\n\nfn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n// Single image\nlet image = DataLoader::new(\"image.jpg\")?.try_read_one()?;\n\n// Nth image from collection\nlet image = DataLoader::new(\"./images/*.jpg\")?.try_read_nth(2)?;\n\n // Range of images\n let images = DataLoader::new(\"./assets\")?.try_read_range(0..5)?;\n\n // All images from mixed sources\n let images = DataLoader::new(vec![\n     \"local.jpg\",\n     \"https://example.com/remote.jpg\",\n     \"./images/*.png\",\n ])?.try_read()?;\n Ok(())\n}\n</code></pre>","text":""},{"location":"guides/dataloader/#video-webcam","title":"Video &amp; Webcam <pre><code>use usls::DataLoader;\n\nfn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\nlet source = \"./assets/bus.jpg | ../video.mp4 | ./assets/*.png | 0\";\nlet dl = DataLoader::new(source)?\n    .with_batch(32)           // 32 images per batch\n    .with_progress_bar(true)  // Show progress\n    .stream()?;               // Start background thread\n\nfor (i, batch) in dl.into_iter().enumerate() {\n    println!(\"Batch {}: {} images\", i, batch.len());\n    // Process batch...\n}\nOk(())\n}\n</code></pre>","text":""},{"location":"guides/dataloader/#configuration","title":"\ud83c\udf9b\ufe0f Configuration","text":"Method Description Default <code>with_batch(n)</code> Set the batch size <code>1</code> <code>with_fps(n)</code> Limit processing speed for videos/streams <code>None</code> <code>with_progress_bar(bool)</code> Show a progress bar in the terminal <code>true</code> <p>TODO</p>"},{"location":"guides/device/","title":"Device Management","text":"<p>usls allows you to precisely control where model inference and image preprocessing occur.</p>"},{"location":"guides/device/#supported-devices","title":"Supported Devices","text":"Device Description Feature <code>Cpu</code> Standard CPU inference (Default) <code>Cuda(id)</code> NVIDIA GPU via CUDA <code>cuda</code> <code>TensorRT(id)</code> NVIDIA GPU via TensorRT <code>tensorrt</code> <code>CoreML</code> Apple Silicon <code>coreml</code> <code>OpenVINO(id)</code> Intel CPU/GPU/VPU <code>openvino</code>"},{"location":"guides/device/#model-vs-processor-device","title":"Model vs. Processor Device","text":"<p>A unique feature of usls is the ability to separate the device for the Model (Inference) and the Processor (Preprocessing).</p> <pre><code>let config = Config::clip()\n    .with_model_device(Device::Cuda(0))     // Inference on GPU\n    .with_processor_device(Device::Cpu)     // Preprocessing on CPU\n    .commit()?;\n</code></pre> <p>Performance Tip</p> <p>For small models, CPU preprocessing is often fast enough. For large batches or high-resolution images, use <code>Device::Cuda(0)</code> for the processor to enable GPU-accelerated resizing and normalization.</p>"},{"location":"guides/device/#multi-gpu-support","title":"Multi-GPU Support","text":"<p>In multi-GPU environments, ensure both the model and the processor use the same GPU ID if they are intended to work together efficiently.</p> <pre><code>let gpu_id = 1;\nconfig.with_model_device(Device::Cuda(gpu_id))\n      .with_processor_device(Device::Cuda(gpu_id));\n</code></pre>"},{"location":"guides/dtype/","title":"DType &amp; Precision","text":"<p>usls supports multiple data types (DTypes) for model inference, allowing you to balance accuracy and performance.</p>"},{"location":"guides/dtype/#supported-dtypes","title":"Supported DTypes","text":"DType Description Best For <code>Fp32</code> 32-bit floating point Maximum accuracy (default) <code>Fp16</code> 16-bit floating point Best performance on modern GPUs <code>Q8</code> 8-bit quantized Reduced memory, high speed <code>Q4F16</code> 4-bit quantized (Fp16) VLM models on restricted memory <code>Bnb4</code> BitsAndBytes 4-bit Ultra-low memory usage"},{"location":"guides/dtype/#configuration","title":"Configuration","text":"<p>You can set the DType for the entire model or per-module:</p> <pre><code>let config = Config::clip()\n    .with_dtype_all(DType::Fp16) // Global\n    .with_visual_dtype(DType::Fp16) // Per-module\n    .commit()?;\n</code></pre> <p>Hardware Support</p> <p>Ensure your chosen Execution Provider supports the target DType. For example, <code>Fp16</code> is highly recommended for TensorRT and CUDA on modern NVIDIA GPUs.</p>"},{"location":"guides/ep/","title":"Execution Providers","text":"<p>usls supports multiple Execution Providers (EPs) via ONNX Runtime to optimize inference on different hardware platforms.</p>"},{"location":"guides/ep/#configuration-patterns","title":"Configuration Patterns","text":"<p>You can configure EPs at different levels of granularity using a predictable naming convention:</p> <ol> <li>Per-Module: <code>with_&lt;module&gt;_&lt;ep&gt;_&lt;field&gt;(value)</code></li> <li>Global (All Modules): <code>with_&lt;ep&gt;_&lt;field&gt;_all(value)</code></li> </ol>"},{"location":"guides/ep/#example-coreml-configuration","title":"Example: CoreML Configuration","text":"<pre><code>Config::dwpose()\n    .with_model_coreml_static_input_shapes(true) // Just for the 'model' module\n    .with_coreml_static_input_shapes_all(true)    // For all modules in this config\n    .commit()?;\n</code></pre>"},{"location":"guides/ep/#tensorrt-optimization","title":"TensorRT Optimization","text":"<p>TensorRT provides the best performance on NVIDIA GPUs but requires an initial \"engine building\" phase.</p>"},{"location":"guides/ep/#dynamic-shapes","title":"Dynamic Shapes","text":"<p>usls uses a syntax that closely mirrors <code>trtexec</code>. You define the minimum, optimal, and maximum shapes for each axis.</p> <p><code>trtexec</code> example:</p> <pre><code>trtexec --fp16 --onnx=your_model.onnx \\\n    --minShapes=images:1x3x416x416 \\\n    --optShapes=images:1x3x640x640 \\\n    --maxShapes=images:8x3x800x800 \\\n    --saveEngine=your_model.engine\n</code></pre> <p>Equivalent <code>usls</code> configuration:</p> <pre><code>Config::yolo()\n    .with_model_ixx(0, 0, (1, 1, 8))        // Axis 0 (Batch): min=1, opt=1, max=8\n    .with_model_ixx(0, 1, 3)                // Axis 1 (Channels): fixed at 3\n    .with_model_ixx(0, 2, (416, 640, 800))  // Axis 2 (Height): min=416, opt=640, max=800\n    .with_model_ixx(0, 3, (416, 640, 800))  // Axis 3 (Width): min=416, opt=640, max=800\n    .commit()?;\n</code></pre> <p>API explanation:</p> <ul> <li><code>with_&lt;module&gt;_ixx(input_idx, axis, (min, opt, max))</code> \u2013 configure dynamic shapes</li> <li><code>input_idx</code>: input tensor index (0-based)</li> <li> <p><code>axis</code>: tensor dimension</p> </li> <li> <p><code>0</code> = batch</p> </li> <li><code>1</code> = channel</li> <li><code>2</code> = height</li> <li><code>3</code> = width</li> <li><code>(min, opt, max)</code>: minimum / optimal / maximum values</li> </ul>"},{"location":"guides/ep/#tensorrt-vs-tensorrt-rtx","title":"TensorRT vs. TensorRT-RTX","text":"<ul> <li>TensorRT: Automatically handles FP32 to FP16 conversion.</li> <li>TensorRT-RTX: Preserves input precision and uses specific RTX optimizations.</li> </ul>"},{"location":"guides/ep/#supported-providers","title":"Supported Providers","text":"Provider Device Feature CUDA <code>Device::Cuda(id)</code> <code>cuda</code> TensorRT <code>Device::TensorRT(id)</code> <code>tensorrt</code> CoreML <code>Device::CoreML</code> <code>coreml</code> OpenVINO <code>Device::OpenVINO(id)</code> <code>openvino</code> DirectML <code>Device::DirectML(id)</code> <code>directml</code> <p>Engine Caching</p> <p>For TensorRT, usls automatically handles engine caching. The first run will be slow while the engine builds, but subsequent runs will start instantly.</p>"},{"location":"guides/modules/","title":"Module System","text":"<p>usls uses a flexible module-based architecture, allowing models to be composed of multiple ONNX components. This is especially important for Vision-Language Models (VLMs) and complex encoder-decoder pipelines.</p>"},{"location":"guides/modules/#the-module-enum","title":"\ud83e\udde9 The <code>Module</code> Enum","text":"<p>All configurable components in usls are identified by the <code>Module</code> enum:</p> <pre><code>pub enum Module {\n    // Standard vision models\n    Model,\n\n    // Vision-Language components\n    Visual,\n    Textual,\n\n    // Encoder-Decoder pipelines\n    Encoder,\n    Decoder,\n    VisualEncoder,\n    TextualEncoder,\n    VisualDecoder,\n    TextualDecoder,\n\n    // Projections &amp; specialized layers\n    VisualProjection,\n    TextualProjection,\n\n    // Custom modules for extensibility\n    Custom(String),\n}\n</code></pre>"},{"location":"guides/modules/#why-use-modules","title":"Why use Modules?","text":"<ol> <li>Granular Control: You can place different modules on different devices (e.g., <code>VisualEncoder</code> on GPU, <code>TextualDecoder</code> on CPU).</li> <li>Mixed Precision: Set <code>DType::Fp16</code> for performance-critical modules while keeping others at <code>DType::Fp32</code>.</li> <li>Optimization: Configure specific execution provider settings per module.</li> </ol>"},{"location":"guides/results/","title":"Result Types (Y)","text":"<p>All inference results in usls are returned in a unified structure called <code>Y</code>. This abstraction allows you to handle results from different models (detection, segmentation, classification) using a consistent API.</p>"},{"location":"guides/results/#the-y-struct","title":"\ud83d\udcca The <code>Y</code> Struct","text":"<p>The <code>Y</code> struct acts as a container for various result types. Most models will populate only one or two fields, while complex models (like YOLOv8-seg) might populate several.</p> <pre><code>pub struct Y {\n    pub hbbs: Vec&lt;Hbb&gt;,             // Horizontal Bounding Boxes\n    pub obbs: Vec&lt;Obb&gt;,             // Oriented Bounding Boxes\n    pub masks: Vec&lt;Mask&gt;,           // Segmentation Masks\n    pub polygons: Vec&lt;Polygon&gt;,     // Contours/Polygons\n    pub keypoints: Vec&lt;Keypoint&gt;,   // Single Keypoint sets\n    pub keypointss: Vec&lt;Vec&lt;Keypoint&gt;&gt;, // Multiple Keypoint sets (Batched)\n    pub probs: Vec&lt;Prob&gt;,           // Classification Probabilities\n    pub texts: Vec&lt;Text&gt;,           // Text outputs (OCR/VLM)\n    pub embedding: X,               // Feature Embeddings\n    pub extras: HashMap&lt;String, X&gt;, // Model-specific custom outputs\n}\n</code></pre>"},{"location":"guides/results/#accessing-results","title":"Accessing Results","text":""},{"location":"guides/results/#borrowing-preferred","title":"Borrowing (Preferred)","text":"<p>Use the accessor methods to borrow data without taking ownership.</p> <pre><code>let ys = model.run(&amp;xs)?;\nfor y in &amp;ys {\n    // Borrow horizontal bounding boxes\n    for hbb in y.hbbs() {\n        println!(\"Box: {:?}, Conf: {}\", hbb.rect(), hbb.conf());\n    }\n}\n</code></pre>"},{"location":"guides/results/#consuming-ownership","title":"Consuming (Ownership)","text":"<p>Access the fields directly if you need to move the data.</p> <pre><code>let ys = model.run(&amp;xs)?;\nfor y in ys {\n    let boxes = y.hbbs; // Moves Vec&lt;Hbb&gt; out of y\n}\n</code></pre>"},{"location":"guides/results/#result-item-details","title":"Result Item Details","text":""},{"location":"guides/results/#bounding-boxes-hbb-obb","title":"Bounding Boxes (Hbb / Obb)","text":"<ul> <li><code>rect()</code>: returns <code>[x1, y1, x2, y2]</code> or oriented corners.</li> <li><code>conf()</code>: detection confidence.</li> <li><code>id()</code>: class index.</li> <li><code>name()</code>: class label.</li> </ul>"},{"location":"guides/results/#masks","title":"Masks","text":"<ul> <li>Typically high-resolution binary or grayscale masks matching the input image size.</li> </ul>"},{"location":"guides/results/#keypoints","title":"Keypoints","text":"<ul> <li>Used for human pose estimation or landmark detection. Contains <code>[x, y, confidence]</code>.</li> </ul> <p>Next: Annotator - Learn how to draw these results.</p>"},{"location":"guides/visualization/","title":"Visualization (Viewer)","text":"<p>The <code>Viewer</code> utility (powered by <code>minifb</code>) provides a simple way to display images, videos, and model results in a real-time window.</p>"},{"location":"guides/visualization/#features","title":"Features","text":"<ul> <li>Cross-Platform: Works on Linux, macOS, and Windows.</li> <li>Window Management: Easy creation and scaling of display windows.</li> <li>Key Event Handling: Capture keyboard input for interactive applications.</li> <li>Video Recording: Record the displayed frames to a video file.</li> </ul>"},{"location":"guides/visualization/#basic-usage","title":"Basic Usage","text":"<pre><code>use usls::*;\n\nlet mut viewer = Viewer::default();\n\nfor xs in &amp;dl {\n    // Check if window was closed by user\n    if viewer.is_window_exist_and_closed() {\n        break;\n    }\n\n    // Display the first image in batch\n    viewer.imshow(&amp;xs[0])?;\n\n    // Wait for key (30ms)\n    if let Some(key) = viewer.wait_key(30) {\n        if key == Key::Escape {\n            break;\n        }\n    }\n}\n</code></pre>"},{"location":"guides/visualization/#window-scaling","title":"Window Scaling","text":"<p>Adjust the window size without changing the underlying image resolution:</p> <p>TODO</p>"},{"location":"guides/visualization/#recording-video","title":"Recording Video","text":"<p>TODO</p> <p>For drawing shapes on images, see the Annotator guide.</p>"},{"location":"model-zoo/overview/","title":"Model Zoo","text":"<p>TODO</p>"},{"location":"model-zoo/vision/","title":"Vision Models","text":"<p>usls supports a wide range of pure vision models for tasks such as object detection, segmentation, pose estimation, and more.</p>"},{"location":"model-zoo/vision/#object-detection-rt-detr-others","title":"Object Detection (RT-DETR &amp; Others)","text":"Model Task / Description Dynamic Batch TensorRT FP32 FP16 RT-DETRv2 Object Detection \u2705 \u2705 \u2705 \u2705 RF-DETR Object Detection \u2705 \u2705 \u2705 \u2705 D-FINE Object Detection \u2705 \u2753 \u2705 \u274c"},{"location":"model-zoo/vision/#image-segmentation-sam-birefnet","title":"Image Segmentation (SAM &amp; BiRefNet)","text":"Model Task / Description Dynamic Batch TensorRT FP32 FP16 SAM Segment Anything \u2705 \u2753 \u2705 \u274c SAM2 Segment Anything \u2705 \u2753 \u2705 \u274c BiRefNet General Segmentation \u2705 \u2753 \u2705 \u2705"},{"location":"model-zoo/vision/#pose-estimation","title":"Pose Estimation","text":"Model Task / Description Dynamic Batch TensorRT FP32 FP16 RTMPose Keypoint Detection \u2705 \u2753 \u2705 \u2705 DWPose Keypoint Detection \u2705 \u2753 \u2705 \u2705"},{"location":"model-zoo/vision/#image-classification-tagging","title":"Image Classification &amp; Tagging","text":"Model Task / Description Dynamic Batch TensorRT FP32 FP16 BEiT Classification \u2705 \u2705 \u2705 \u2705 RAM++ Image Tagging \u2705 \u2753 \u2705 \u2705 <p>For VLM models, see the VLM Models page.</p>"},{"location":"model-zoo/vlm/","title":"VLM Models","text":"<p>usls supports state-of-the-art Vision-Language Models (VLM) for tasks like image captioning, visual question answering (VQA), and open-set object detection.</p>"},{"location":"model-zoo/vlm/#multi-modal-models","title":"Multi-Modal Models","text":"Model Task / Description Dynamic Batch FP32 FP16 Q4f16 Florence2 General Vision Tasks \u2705 \u2705 \u2705 \u274c Moondream2 VQA, Captioning \u2705 \u274c \u274c \u2705 SmolVLM2 Lightweight VQA \u2705 \u2705 \u2753 \u2753 BLIP Image Captioning \u2705 \u2705 \u2753 \u274c"},{"location":"model-zoo/vlm/#open-set-detection-segmentation","title":"Open-Set Detection &amp; Segmentation","text":"Model Task / Description Dynamic Batch FP16 Q8 GroundingDINO Text-to-Bbox \u2705 \u2705 \u2705 OWLv2 Open-World Detection \u2705 \u2705 \u274c YOLO-World Real-time Open-Set \u2705 \u2705 \u2705"},{"location":"model-zoo/vlm/#embedding-models","title":"Embedding Models","text":"Model Task / Description Dynamic Batch FP16 CLIP Vision-Language Embedding \u2705 \u2705 jina-clip-v2 Enhanced Embedding \u2705 \u2705 DINOv3 Vision Features \u2705 \u2705 <p>See the Getting Started guide to run these models.</p>"},{"location":"model-zoo/yolo/","title":"YOLO Series","text":"<p>usls provides comprehensive support for the YOLO (You Only Look Once) family of models, spanning from YOLOv5 to the latest YOLO26.</p>"},{"location":"model-zoo/yolo/#supported-versions","title":"Supported Versions","text":"Model Task / Description Dynamic Batch TensorRT FP16 Q8 Q4f16 YOLOv5 Classification, Detection, Segmentation \u2705 \u2705 \u2705 \u2705 \u274c YOLOv8 Detection, Segmentation, Pose, OBB \u2705 \u2705 \u2705 \u2705 \u274c YOLO11 Detection, Segmentation, Pose, OBB \u2705 \u2705 \u2705 \u2705 \u274c YOLOv12 Detection, Segmentation, Classification \u2705 \u2705 \u2705 \u2705 \u2705 YOLO26 Detection, Segmentation, Pose, OBB \u2705 \u2705 \u2705 \u2705 \u2705"},{"location":"model-zoo/yolo/#quick-usage","title":"Quick Usage","text":"<pre><code># Run YOLOv11 object detection on CPU\ncargo run -r --example yolo -- --task detect --ver 11 --scale n\n</code></pre>"},{"location":"model-zoo/yolo/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>For detailed performance metrics across different hardware (CUDA, TensorRT, CPU), please refer to the Home Page.</p>"}]}