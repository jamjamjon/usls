<h2 align="center">usls</h2>
<p align="center">
<a href="https://github.com/jamjamjon/usls/actions/workflows/rust-ci.yml">
        <img src="https://github.com/jamjamjon/usls/actions/workflows/rust-ci.yml/badge.svg" alt="Rust CI">
    </a>
    <a href='https://crates.io/crates/usls'>
        <img src='https://img.shields.io/crates/v/usls?logo=rust&logoColor=white' alt='Crates.io Version'>
    </a>
    <a href='https://github.com/microsoft/onnxruntime/releases'>
        <img src='https://img.shields.io/badge/onnxruntime-%3E%3D%201.22.0-3399FF?logo=onnx&logoColor=white' alt='ONNXRuntime MSRV'>
    </a>
    <a href='https://crates.io/crates/usls'>
        <img src='https://img.shields.io/crates/msrv/usls?color=yellow&logo=rust' alt='Rust MSRV'>
    </a>
</p>


<br/>

<p align="center">
  <a href="https://jamjamjon.github.io/usls/">ğŸ“˜ <strong>Documentation</strong></a> | 
  <a href="./examples/README.md">ğŸŒŸ <strong>Examples</strong></a> 
</p>

<br/>

**usls** is a cross-platform Rust library powered by ONNX Runtime for efficient inference of SOTA vision and vision-language models (***typically under 1B parameters***).

<p align="center">
<img src='https://github.com/jamjamjon/assets/releases/download/images/pipeline.png'>
(Generated by Seedream4.5)
</p>


## ğŸŒŸ Highlights

- **âš¡ High Performance**: Multi-threading, SIMD, and CUDA-accelerated processing
- **ğŸŒ Cross-Platform**: Linux, macOS, Windows with ONNX Runtime execution providers (CUDA, TensorRT, CoreML, OpenVINO, DirectML, etc.)
- **ğŸ—ï¸ Unified API**: Single `Model` trait inference with `run()`/`forward()`/`encode_images()`/`encode_texts()` and unified `Y` output
- **ğŸ“¥ Auto-Management**: Automatic model download (HuggingFace/GitHub), caching and path resolution
- **ğŸ“¦ Multiple Inputs**: Image, directory, video, webcam, stream and combinations
- **ğŸ¯ Precision Support**: FP32, FP16, INT8, UINT8, Q4, Q4F16, BNB4, and more
- **ğŸ› ï¸ Full-Stack Suite**: `DataLoader`, `Annotator`, and `Viewer` for complete workflows
- **ğŸŒ± Model Ecosystem**: 50+ SOTA vision and VLM models

## ğŸš€ Quick Start

Run the **YOLO-Series demo** to explore models with different tasks, precision and execution providers:

- **Tasks**: `detect`, `segment`, `pose`, `classify`, `obb`
- **Versions**: `YOLOv5`, `YOLOv6`, `YOLOv7`, `YOLOv8`, `YOLOv9`, `YOLOv10`, `YOLO11`, `YOLOv12`, `YOLOv13`, `YOLO26`
- **Scales**: `n`, `s`, `m`, `l`, `x`
- **Precision**: `fp32`, `fp16`, `q8`, `q4`, `q4f16`, `bnb4`
- **Execution Providers**: `CPU`, `CUDA`, `TensorRT`, `TensorRT-RTX`, `CoreML`, `OpenVINO`, and more


```bash
# CPU: Object detection with YOLO26n (FP16)
cargo run -r --example yolo -- --task detect --ver 26 --scale n --dtype fp16

# CUDA model + CPU processor: Instance segmentation with YOLO11m
cargo run -r -F cuda --example yolo -- --task segment --ver 11 --scale m --device cuda:0 --processor-device cpu

# CUDA model + CUDA processor: Pose estimation with YOLOv8m
cargo run -r -F cuda-full --example yolo -- --task pose --ver 8 --scale s --device cuda:0 --processor-device cuda:0

# TensorRT model + CPU processor
cargo run -r -F tensorrt --example yolo -- --device tensorrt:0 --processor-device cpu

# TensorRT model + CUDA processor (CUDA 12.4)
cargo run -r -F tensorrt-cuda-12040 --example yolo -- --device tensorrt:0 --processor-device cuda:0

# TensorRT-RTX model + CUDA processor
cargo run -r -F nvrtx-full --example yolo -- --device nvrtx:0 --processor-device cuda:0

# TensorRT-RTX model + CPU processor
cargo run -r -F nvrtx --example yolo -- --device nvrtx:0

# Apple Silicon CoreML
cargo run -r -F coreml --example yolo -- --device coreml

# Intel OpenVINO (CPU/GPU/VPU)
cargo run -r -F openvino -F ort-load-dynamic --example yolo -- --device openvino:CPU

# Show all available options
cargo run -r --example yolo -- --help
```



### Performance

>**Environment:** NVIDIA RTX 3060Ti (TensorRT-10.11.0.33, CUDA 12.8, TensorRT-RTX-1.3.0.35) / Intel i5-12400F  
>
>**Setup:** YOLO26n, COCO2017 validation set (5,000 images), Resolution: 640x640, Conf thresholds: [0.35, 0.3, ..]
>
> ***Results are for rough reference only.***


| EP | Image<br>Processor | DType | Batch | Preprocess | Inference | Postprocess | Total |
| --- | --- | --- | --- | --- | --- | --- | --- |
| TensorRT | CUDA | FP16 | 1 | ~233Âµs | ~1.3ms | ~14Âµs | **~1.55ms** |
| TensorRT-RTX | CUDA | FP32 | 1 | ~233Âµs | ~2.0ms | ~10Âµs | **~2.24ms** |
| TensorRT-RTX | CUDA | FP16 | 1 | â“ | â“ | â“ | â“ |
| CUDA | CUDA | FP32 | 1 | ~233Âµs | ~5.0ms | ~17Âµs | **~5.25ms** |
| CUDA | CUDA | FP16 | 1 | ~233Âµs | ~3.6ms | ~17Âµs | **~3.85ms** |
| CUDA | CPU | FP32 | 1 | ~800Âµs | ~6.5ms | ~14Âµs | **~7.31ms** |
| CUDA | CPU | FP16 | 1 | ~800Âµs | ~5.0ms | ~14Âµs | **~5.81ms** |
| CPU | CPU | FP32 | 1 | ~970Âµs | ~20.5ms | ~14Âµs | **~21.48ms** |
| CPU | CPU | FP16 | 1 | ~970Âµs | ~25.0ms | ~14Âµs | **~25.98ms** |
| TensorRT | CUDA | FP16 | **8** | ~1.2ms | ~6.0ms | ~55Âµs | **~7.26ms** |
| TensorRT | CPU | FP16 | **8** | ~18.0ms | ~25.5ms | ~55Âµs | **~43.56ms** |


## Documentation

- <a href="https://jamjamjon.github.io/usls/">ğŸ“– <strong>Documentation</strong></a>
- <a href="./examples/README.md">ğŸŒŸ <strong>Examples</strong></a> 


## ğŸ¤ Contributing

This is a personal project maintained in spare time, so progress on performance optimization and new model support may vary.

**We highly welcome PRs for model optimization!** If you have expertise in specific models and can help optimize their interfaces or post-processing, your contributions would be invaluable. Feel free to open an issue or submit a pull request for suggestions, bug reports, or new features.

## ğŸ™ Acknowledgments

- This project is built on top of [ort (ONNX Runtime for Rust)](https://github.com/pykeio/ort), which provides seamless Rust bindings for [ONNX Runtime](https://github.com/microsoft/onnxruntime). Special thanks to the `ort` maintainers.

- Special thanks to [@kadu-v](https://github.com/kadu-v) for the [jamtrack-rs](https://github.com/kadu-v/jamtrack-rs) project, which inspired our ByteTracker implementation


Thanks to all the open-source libraries and their maintainers that make this project possible. See [Cargo.toml](Cargo.toml) for a complete list of dependencies.

## ğŸ“œ License

This project is licensed under [LICENSE](LICENSE).
