<h2 align="center">usls</h2>
<p align="center">
<a href="https://github.com/jamjamjon/usls/actions/workflows/rust-ci.yml">
        <img src="https://github.com/jamjamjon/usls/actions/workflows/rust-ci.yml/badge.svg" alt="Rust CI">
    </a>
    <a href='https://crates.io/crates/usls'>
        <img src='https://img.shields.io/crates/v/usls?logo=rust&logoColor=white' alt='Crates.io Version'>
    </a>
    <a href='https://github.com/microsoft/onnxruntime/releases'>
        <img src='https://img.shields.io/badge/onnxruntime-%3E%3D%201.22.0-3399FF?logo=onnx&logoColor=white' alt='ONNXRuntime MSRV'>
    </a>
    <a href='https://crates.io/crates/usls'>
        <img src='https://img.shields.io/crates/msrv/usls?color=yellow&logo=rust' alt='Rust MSRV'>
    </a>
</p>


<br/>

<p align="center">
  <a href="https://docs.rs/usls/latest/usls/">ğŸ“˜ <strong>API Documentation</strong></a> | 
  <a href="./examples/README.md">ğŸŒŸ <strong>Examples</strong></a> |  
  <a href="#-model-zoo">ğŸ“¦ <strong>Model Zoo</strong></a>
</p>

<br/>

**usls** is a cross-platform Rust library powered by ONNX Runtime for efficient inference of SOTA vision and vision-language models (***typically under 1B parameters***).

<p align="center">
<img src='https://github.com/jamjamjon/assets/releases/download/images/pipeline.png'>
(Generated by Seedream4.5)
</p>


## ğŸŒŸ Highlights

- **âš¡ High Performance**: Multi-threading, SIMD, and CUDA-accelerated processing
- **ğŸŒ Cross-Platform**: Linux, macOS, Windows with ONNX Runtime execution providers (CUDA, TensorRT, CoreML, OpenVINO, DirectML, etc.)
- **ğŸ—ï¸ Unified API**: Single `Model` trait inference with `run()`/`forward()`/`encode_images()`/`encode_texts()` and unified `Y` output
- **ğŸ“¥ Auto-Management**: Automatic model download (HuggingFace/GitHub), caching and path resolution
- **ğŸ“¦ Multiple Inputs**: Image, directory, video, webcam, stream and combinations
- **ğŸ¯ Precision Support**: FP32, FP16, INT8, UINT8, Q4, Q4F16, BNB4, and more
- **ğŸ› ï¸ Full-Stack Suite**: `DataLoader`, `Annotator`, and `Viewer` for complete workflows
- **ğŸŒ± Model Ecosystem**: 50+ SOTA vision and VLM models

## ğŸš€ Quick Start

Run the **YOLO-Series demo** to explore models with different tasks, precision and execution providers:

- **Tasks**: `detect`, `segment`, `pose`, `classify`, `obb`
- **Versions**: `YOLOv5`, `YOLOv6`, `YOLOv7`, `YOLOv8`, `YOLOv9`, `YOLOv10`, `YOLO11`, `YOLOv12`, `YOLOv13`, `YOLO26`
- **Scales**: `n`, `s`, `m`, `l`, `x`
- **Precision**: `fp32`, `fp16`, `q8`, `q4`, `q4f16`, `bnb4`
- **Execution Providers**: `CPU`, `CUDA`, `TensorRT`, `TensorRT-RTX`, `CoreML`, `OpenVINO`, and more


### Examples

```bash
# CPU: Object detection with YOLO26n (FP16)
cargo run -r --example yolo -- --task detect --ver 26 --scale n --dtype fp16

# CUDA model + CPU processor: Instance segmentation with YOLO11m
cargo run -r -F cuda --example yolo -- --task segment --ver 11 --scale m --device cuda:0 --processor-device cpu

# CUDA model + CUDA processor: Pose estimation with YOLOv8m
cargo run -r -F cuda-full --example yolo -- --task pose --ver 8 --scale s --device cuda:0 --processor-device cuda:0

# TensorRT model + CPU processor
cargo run -r -F tensorrt --example yolo -- --device tensorrt:0 --processor-device cpu

# TensorRT model + CUDA processor (CUDA 12.4)
cargo run -r -F tensorrt-cuda-12040 --example yolo -- --device tensorrt:0 --processor-device cuda:0

# TensorRT-RTX model + CUDA processor
cargo run -r -F nvrtx-full --example yolo -- --device nvrtx:0 --processor-device cuda:0

# TensorRT-RTX model + CPU processor
cargo run -r -F nvrtx --example yolo -- --device nvrtx:0

# Apple Silicon CoreML
cargo run -r -F coreml --example yolo -- --device coreml

# Intel OpenVINO (CPU/GPU/VPU)
cargo run -r -F openvino -F ort-load-dynamic --example yolo -- --device openvino:CPU

# Show all available options
cargo run -r --example yolo -- --help
```
>
> See [YOLO Examples](./examples/yolo/README.md) for more details and use cases.
>
> See [Device Combination Guide](#-device-combination-guide) for feature and device configurations.



### Performance

>**Environment:** NVIDIA RTX 3060Ti (TensorRT-10.11.0.33, CUDA 12.8, TensorRT-RTX-1.3.0.35) / Intel i5-12400F  
>
>**Setup:** YOLO26n, COCO2017 validation set (5,000 images), Resolution: 640x640, Conf thresholds: [0.35, 0.3, ..]
>
> ***Results are for rough reference only.***


| EP | Image<br>Processor | DType | Batch | Preprocess | Inference | Postprocess | Total |
| --- | --- | --- | --- | --- | --- | --- | --- |
| TensorRT | CUDA | FP16 | 1 | ~233Âµs | ~1.3ms | ~14Âµs | **~1.55ms** |
| TensorRT-RTX | CUDA | FP32 | 1 | ~233Âµs | ~2.0ms | ~10Âµs | **~2.24ms** |
| TensorRT-RTX | CUDA | FP16 | 1 | â“ | â“ | â“ | â“ |
| CUDA | CUDA | FP32 | 1 | ~233Âµs | ~5.0ms | ~17Âµs | **~5.25ms** |
| CUDA | CUDA | FP16 | 1 | ~233Âµs | ~3.6ms | ~17Âµs | **~3.85ms** |
| CUDA | CPU | FP32 | 1 | ~800Âµs | ~6.5ms | ~14Âµs | **~7.31ms** |
| CUDA | CPU | FP16 | 1 | ~800Âµs | ~5.0ms | ~14Âµs | **~5.81ms** |
| CPU | CPU | FP32 | 1 | ~970Âµs | ~20.5ms | ~14Âµs | **~21.48ms** |
| CPU | CPU | FP16 | 1 | ~970Âµs | ~25.0ms | ~14Âµs | **~25.98ms** |
| TensorRT | CUDA | FP16 | **8** | ~1.2ms | ~6.0ms | ~55Âµs | **~7.26ms** |
| TensorRT | CPU | FP16 | **8** | ~18.0ms | ~25.5ms | ~55Âµs | **~43.56ms** |





## ğŸ“¦ Model Zoo

> **Status:**â€‚âœ… **Supported**â€‚â€‚|â€‚â€‚â“ **Unknown**â€‚â€‚|â€‚â€‚âŒ **Not Supported For Now**

<details closed>
<summary><b>ğŸ”¥ YOLO-Series</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [YOLOv5](https://github.com/ultralytics/yolov5) | Image Classification<br />Object Detection<br />Instance Segmentation | [demo](./examples/yolo) | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | 
| [YOLOv6](https://github.com/meituan/YOLOv6) | Object Detection | [demo](./examples/yolo) | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | 
| [YOLOv7](https://github.com/WongKinYiu/yolov7) | Object Detection | [demo](./examples/yolo) | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | 
| [YOLOv8](https://github.com/ultralytics/ultralytics) | Object Detection<br />Instance Segmentation<br />Image Classification<br />Oriented Object Detection<br />Keypoint Detection | [demo](./examples/yolo) | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | 
| [YOLO11](https://github.com/ultralytics/ultralytics) | Object Detection<br />Instance Segmentation<br />Image Classification<br />Oriented Object Detection<br />Keypoint Detection | [demo](./examples/yolo) | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | 
| [YOLOv9](https://github.com/WongKinYiu/yolov9) | Object Detection | [demo](./examples/yolo) | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | 
| [YOLOv10](https://github.com/THU-MIG/yolov10) | Object Detection | [demo](./examples/yolo) | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | 
| [YOLOv12](https://github.com/sunsmarterjie/yolov12) | Image Classification<br />Object Detection<br />Instance Segmentation | [demo](./examples/yolo) | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | 
| [YOLOv13](https://github.com/iMoonLab/yolov13) | Object Detection | [demo](./examples/yolo) | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | 
| [YOLO26](https://github.com/ultralytics/ultralytics) | Object Detection<br />Instance Segmentation<br />Image Classification<br />Oriented Object Detection<br />Keypoint Detection | [demo](./examples/yolo) | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | 

</details>


<details closed>
<summary><b>ğŸ·ï¸ Image Classification & Tagging</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [BEiT](https://github.com/microsoft/unilm/tree/master/beit) | Image Classification | [demo](./examples/image-classification) | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | 
| [ConvNeXt](https://github.com/facebookresearch/ConvNeXt) | Image Classification | [demo](./examples/image-classification) | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | 
| [FastViT](https://github.com/apple/ml-fastvit) | Image Classification | [demo](./examples/image-classification) | âœ… | âœ…| âœ… | âœ…| âŒ | âŒ | âŒ | 
| [MobileOne](https://github.com/apple/ml-mobileone) | Image Classification | [demo](./examples/image-classification) | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | 
| [DeiT](https://github.com/facebookresearch/deit) | Image Classification | [demo](./examples/image-classification) | âœ… | âœ… | âœ… | âœ…| âŒ | âŒ | âŒ | 
| [RAM](https://github.com/xinyu1205/recognize-anything) | Image Tagging | [demo](./examples/image-classification) | âœ… | â“| âœ… | âœ…| âœ… | âœ… | âœ… |
| [RAM++](https://github.com/xinyu1205/recognize-anything) | Image Tagging | [demo](./examples/image-classification) | âœ… | â“ | âœ… | âœ… | âœ… | âœ… | âœ… |

</details>


<details closed>
<summary><b>ğŸ¯ Object Detection</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [RT-DETRv1](https://github.com/lyuwenyu/RT-DETR) | Object Detection | [demo](./examples/object-detection) | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | 
| [RT-DETRv2](https://github.com/lyuwenyu/RT-DETR) | Object Detection | [demo](./examples/object-detection) | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ…  | 
| [RT-DETRv4](https://github.com/RT-DETRs/RT-DETRv4) | Object Detection | [demo](./examples/object-detection) | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ…  | 
| [RF-DETR](https://github.com/roboflow/rf-detr) | Object Detection | [demo](./examples/object-detection)| âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | 
| [PP-PicoDet](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.8/configs/picodet) | Object Detection | [demo](./examples/ocr) | âŒ | â“  | âœ… | âŒ | âŒ | âŒ | âŒ | 
| [D-FINE](https://github.com/manhbd-22022602/D-FINE) | Object Detection | [demo](./examples/object-detection) | âœ… | â“  | âœ…  | âŒ | âŒ | âŒ | âŒ | âŒ | 
| [DEIM](https://github.com/ShihuaHuang95/DEIM) | Object Detection | [demo](./examples/object-detection) | âœ… | â“  | âœ…  | âŒ | âŒ | âŒ | âŒ | âŒ | 
| [DEIMv2](https://github.com/Intellindust-AI-Lab/DEIMv2) | Object Detection | [demo](./examples/object-detection) | âœ… | â“  |âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | 

</details>

<details closed>
<summary><b>ğŸ¨ Image Segmentation</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [SAM](https://github.com/facebookresearch/segment-anything) | Segment Anything | [demo](./examples/image-segmentation) | âœ… | â“ | âœ… | âŒ | âŒ | âŒ | âŒ |  
| [SAM-HQ](https://github.com/SysCV/sam-hq) | Segment Anything | [demo](./examples/image-segmentation) | âœ… | â“ | âœ… | âŒ | âŒ | âŒ | âŒ | 
| [MobileSAM](https://github.com/ChaoningZhang/MobileSAM) | Segment Anything | [demo](./examples/image-segmentation) | âœ… |â“ | âœ… | âŒ | âŒ | âŒ | âŒ |  
| [EdgeSAM](https://github.com/chongzhou96/EdgeSAM) | Segment Anything | [demo](./examples/image-segmentation) | âœ… | â“ | âœ… | âŒ | âŒ | âŒ | âŒ | 
| [FastSAM](https://github.com/CASIA-IVA-Lab/FastSAM) | Instance Segmentation | [demo](./examples/image-segmentation) | âœ… | âœ… | âœ…  | âœ…  | âœ…  | âœ…  | âœ…  |
| [SAM2](https://github.com/facebookresearch/segment-anything-2) | Segment Anything | [demo](./examples/image-segmentation) | âœ… | â“ | âœ… | âŒ | âŒ  | âŒ | âŒ |  
| [SAM3-Tracker](https://github.com/facebookresearch/segment-anything-3) | Segment Anything | [demo](./examples/image-segmentation) | âœ… | âœ… | âœ…  | âœ…  | âœ…  | âœ…  | âœ…  |
| [BiRefNet - COD](https://github.com/ZhengPeng7/BiRefNet) | Camouflaged Object Detection | [demo](./examples/birefnet) | âœ… | â“ | âœ… | âœ… | âœ… | âœ… |  âœ… |
| [BiRefNet - DIS](https://github.com/ZhengPeng7/BiRefNet) | Dichotomous Image Segmentation | [demo](./examples/birefnet) | âœ… | â“ | âœ… | âœ… | âœ… | âœ… |  âœ… |
| [BiRefNet - HRSOD](https://github.com/ZhengPeng7/BiRefNet) | High-Resolution Salient Object Detection | [demo](./examples/birefnet) | âœ… | â“ | âœ… | âœ… | âœ… | âœ… |  âœ… |
| [BiRefNet - Massive](https://github.com/ZhengPeng7/BiRefNet) | Multi-Dataset Robust Segmentation | [demo](./examples/birefnet) | âœ… | â“ | âœ… | âœ… | âœ… | âœ… |  âœ… |


</details>

<details closed>
<summary><b>âœ¨ Background Removal</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [RMBG](https://huggingface.co/briaai/RMBG-2.0) | Image Segmentation<br />Background Removal | [demo](./examples/background-removal) | âœ… | â“ |âœ…  | âœ…  | âœ…  | âœ…  | âœ…  |
| [BEN2](https://huggingface.co/PramaLLC/BEN2) | Image Segmentation<br />Background Removal | [demo](./examples/background-removal) | âœ… | â“ |âœ…  | âœ…  | âŒ | âŒ | âŒ |

</details>

<details closed>
<summary><b>âœ‚ï¸ Image Matting & Portrait Segmentation</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [MODNet](https://github.com/ZHKKKe/MODNet) | Image Matting | [demo](./examples/image-matting) | âœ… |  â“ | âœ…  | âœ…  | âœ…  | âŒ  | âŒ  |
| [MediaPipe Selfie](https://ai.google.dev/edge/mediapipe/solutions/vision/image_segmenter) | Image Segmentation | [demo](./examples/image-matting) | âœ… |  â“ | âœ…  | âœ…  | âœ…  | âŒ  | âŒ  |
| [BiRefNet - Portrait](https://github.com/ZhengPeng7/BiRefNet) | Portrait Background Removal | [demo](./examples/birefnet) | âœ… | â“ | âœ… | âœ… | âœ… | âœ… |  âœ… |
| [BiRefNet - Matting](https://github.com/ZhengPeng7/BiRefNet) | Portrait Matting & Background Removal | [demo](./examples/birefnet) | âœ… | â“ | âœ… | âœ… | âœ… | âœ… |  âœ… |
| [BiRefNet - HR Matting](https://github.com/ZhengPeng7/BiRefNet) | High-Resolution Portrait Matting | [demo](./examples/birefnet) | âœ… | â“ | âœ… | âœ… | âœ… | âœ… |  âœ… |
| [BiRefNet - General](https://github.com/ZhengPeng7/BiRefNet) | General Purpose Segmentation | [demo](./examples/birefnet) | âœ… | â“ | âœ… | âœ… | âœ… | âœ… |  âœ… |
| [BiRefNet - HR General](https://github.com/ZhengPeng7/BiRefNet) | High-Resolution General Segmentation | [demo](./examples/birefnet) | âœ… | â“ | âœ… | âœ… | âœ… | âœ… |  âœ… |
| [BiRefNet - Lite General](https://github.com/ZhengPeng7/BiRefNet) | Lightweight General Segmentation (2K) | [demo](./examples/birefnet) | âœ… | â“ | âœ… | âœ… | âœ… | âœ… |  âœ… |
| [BiRefNet - General Tiny](https://github.com/ZhengPeng7/BiRefNet) | Lightweight General Segmentation with Swin-V1-Tiny | [demo](./examples/birefnet) | âœ… | â“ | âœ… | âœ… | âœ… | âœ… |  âœ… |

</details>



<details closed>
<summary><b>ğŸ—ºï¸ Open-Set Detection & Segmentation</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [GroundingDINO](https://github.com/IDEA-Research/GroundingDINO) | Open-Set Detection With Language | [demo](./examples/open-set-detection) | âœ… | â“ | âœ…  | âœ…  | âœ…  | âœ…  | âœ…  |
| [MM-GDINO](https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/README.md) | Open-Set Detection With Language | [demo](./examples/open-set-detection) | âœ… | â“ | âœ…  | âœ…  | âœ…  | âœ…  | âœ…  |
| [LLMDet](https://github.com/iSEE-Laboratory/LLMDet) | Open-Set Detection With Language | [demo](./examples/open-set-detection) | âœ… | â“ | âœ…  | âœ…  | âœ…  | âœ…  | âœ…  |
| [OWLv2](https://huggingface.co/google/owlv2-base-patch16-ensemble) | Open-Set Object Detection | [demo](./examples/open-set-detection) | âœ… | â“  | âœ… | âœ…   | âŒ | âŒ | âŒ |
| [YOLO-World](https://github.com/AILab-CVC/YOLO-World) | Open-Set Detection With Language | [demo](./examples/yolo) | âœ… | âœ… |âœ…  | âœ…  | âœ…  | âœ…  | âœ…  |
| [YOLOE](https://github.com/THU-MIG/yoloe) | Open-Set Detection And Segmentation | [demo](./examples/open-set-segmentation) | âœ… | âœ… | âœ…  | âœ…  | âœ…  | âœ…  | âœ…  |
| [SAM3-Image](https://github.com/facebookresearch/segment-anything-3) | Open-Set Detection And Segmentation| [demo](./examples/open-set-segmentation) | âœ… | âœ… | âœ…  | âœ…  | âœ…  | âœ…  | âœ…  |

</details>



<details closed>
<summary><b>ğŸƒ Multi-Object Tracking</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [ByteTrack](https://github.com/FoundationVision/ByteTrack) | Multi-Object Tracking | [demo](./examples/mot) | âŒ | âŒ  | âŒ | âŒ | âŒ | âŒ | âŒ |

</details>


<details closed>
<summary><b>ğŸ’ Image Super-Resolution</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [Swin2SR](https://github.com/mv-lab/swin2sr) | Image Restoration | [demo](./examples/super-resolution) | âœ… |  â“ | âœ…  | âœ…  | âœ…  | âœ…  | âœ…  |
| [APISR](https://github.com/Kiteretsu77/APISR) | Anime Super-Resolution | [demo](./examples/super-resolution) | âœ… |  â“ | âœ…  | âœ…  | âœ…  | âœ…  | âœ…  |

</details>


<details closed>
<summary><b>ğŸ¤¸ Pose Estimation</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [RTMPose](https://github.com/open-mmlab/mmpose/tree/dev-1.x/projects/rtmpose) | Keypoint Detection | [demo](./examples/pose-estimation) | âœ… | â“ |  âœ…  | âœ…  | âœ…  | âœ…  | âœ…  |
| [DWPose](https://github.com/IDEA-Research/DWPose) | Keypoint Detection | [demo](./examples/pose-estimation) | âœ… | â“ |  âœ…  | âœ…  | âœ…  | âœ…  | âœ…  |
| [RTMW](https://arxiv.org/abs/2407.08634) | Keypoint Detection | [demo](./examples/pose-estimation) | âœ… | â“ |  âœ…  | âœ…  | âœ…  | âœ…  | âœ…  |
| [RTMO](https://github.com/open-mmlab/mmpose/tree/main/projects/rtmo) | Keypoint Detection | [demo](./examples/pose-estimation) | âœ… | â“ |  âœ…  | âœ…  | âœ…  | âœ…  | âŒ  |

</details>

<details closed>
<summary><b>ğŸ” OCR & Document Understanding</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [DB](https://arxiv.org/abs/1911.08947) | Text Detection | [demo](./examples/ocr) | âœ… |  â“ | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ |
| [FAST](https://github.com/czczup/FAST) | Text Detection | [demo](./examples/ocr) | âœ… |  â“ | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ |
| [LinkNet](https://arxiv.org/abs/1707.03718) | Text Detection | [demo](./examples/ocr) | âœ… |  â“ | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ |
| [SVTR](https://arxiv.org/abs/2205.00159) | Text Recognition | [demo](./examples/ocr) | âœ… | â“ | âœ… | âœ… | âŒ | âŒ | âŒ |
| [TrOCR](https://huggingface.co/microsoft/trocr-base-printed) | Text Recognition | [demo](./examples/ocr) | âœ… | â“ | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ |
| [SLANet](https://paddlepaddle.github.io/PaddleOCR/latest/algorithm/table_recognition/algorithm_table_slanet.html) | Table Recognition | [demo](./examples/ocr) | âœ… | â“ | âœ… | âœ… | âŒ | âŒ | âŒ |
| [DocLayout-YOLO](https://github.com/opendatalab/DocLayout-YOLO) | Object Detection | [demo](./examples/ocr) | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ |

</details>

<details closed>
<summary><b>ğŸ§© Vision-Language Models (VLM)</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [BLIP](https://github.com/salesforce/BLIP) | Image Captioning | [demo](./examples/vlm) | âœ… | â“ | âœ… |â“ | âŒ | âŒ | âŒ |
| [Florence2](https://arxiv.org/abs/2311.06242) | A Variety of Vision Tasks | [demo](./examples/vlm) | âœ… | â“ | âœ… |âœ…  | âŒ | âŒ | âŒ |
| [Moondream2](https://github.com/vikhyat/moondream/tree/main) | Open-Set Object Detection<br />Open-Set Keypoints Detection<br />Image Captioning<br />Visual Question Answering | [demo](./examples/vlm) | âœ… | â“  | âŒ | âŒ |âœ…  | âœ…  | âŒ |
| [SmolVLM](https://huggingface.co/HuggingFaceTB/SmolVLM-256M-Instruct) | Visual Question Answering | [demo](./examples/vlm) | âœ… | â“| âœ… | â“ | â“ | â“ | â“ | 
| [SmolVLM2](https://huggingface.co/HuggingFaceTB/SmolVLM-256M-Instruct) | Visual Question Answering | [demo](./examples/vlm) | âœ… | â“| âœ… | â“ | â“ | â“ | â“ | 
| [FastVLM](https://github.com/apple/ml-fastvlm) | Vision Language Models | [demo](./examples/vlm) | âœ… |  â“ | âœ…  | âœ…|âœ…  | âœ… | âœ… |

</details>


<details closed>
<summary><b>ğŸ§¬ Embedding Model</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [CLIP](https://github.com/openai/CLIP) | Vision-Language Embedding | [demo](./examples/embedding) | âœ… | â“ | âœ…  | âœ…|âœ…  | âœ… | âœ… |
| [jina-clip-v1](https://huggingface.co/jinaai/jina-clip-v1) | Vision-Language Embedding | [demo](./examples/embedding) | âœ… | â“ | âœ…  | âœ…|âœ…  | âœ… | âœ… |
| [jina-clip-v2](https://huggingface.co/jinaai/jina-clip-v2) | Vision-Language Embedding | [demo](./examples/embedding) | âœ… | â“ | âœ…  | âœ…|âœ…  | âœ… | âœ… |
| [mobileclip](https://github.com/apple/ml-mobileclip) | Vision-Language Embedding | [demo](./examples/embedding) | âœ… | â“ | âœ…  | âœ…|âœ…  | âœ… | âœ… |
| [DINOv2](https://github.com/facebookresearch/dinov2) | Vision Embedding | [demo](./examples/embedding) | âœ… | â“ | âœ…  | âŒ|âŒ  | âŒ | âŒ |
| [DINOv3](https://github.com/facebookresearch/dinov3) | Vision Embedding | [demo](./examples/embedding) | âœ… | â“ | âœ…  | âœ…|âœ…  | âœ… | âœ… |
</details>

<details closed>
<summary><b>ğŸ“ Depth Estimation</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [DepthAnything v1](https://github.com/LiheYoung/Depth-Anything) | Monocular Depth Estimation | [demo](./examples/depth-estimation) | âœ… | â“ | âœ…  | âœ…|âœ…  | âœ… | âœ… |
| [DepthAnything v2](https://github.com/LiheYoung/Depth-Anything) | Monocular Depth Estimation | [demo](./examples/depth-estimation) | âœ… |  â“ | âœ…  | âœ…|âœ…  | âœ… | âœ… |
| [DepthPro](https://github.com/apple/ml-depth-pro) | Monocular Depth Estimation | [demo](./examples/depth-estimation) | âœ… | â“ | âœ…  | âœ…|âœ…  | âœ… | âœ… |
| [Depth-Anything-3](https://github.com/ByteDance-Seed/Depth-Anything-3) | Monocular<br/>Metric<br/>Multi-View | [demo](./examples/depth-estimation) | âœ… | â“ | âœ…  | âœ…|âœ…  | âœ… | âœ… |

</details>


<details closed>
<summary><b>ğŸŒŒ Others</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [Sapiens](https://github.com/facebookresearch/sapiens/tree/main) | Foundation for Human Vision Models | [demo](./examples/sapiens) | âœ… | â“ | âœ…  | âœ…|âœ…  | âœ… | âœ… |
| [YOLOPv2](https://arxiv.org/abs/2208.11434) | Panoptic Driving | [demo](./examples/image-segmentation) | âœ… | â“ | âœ… | âŒ| âŒ  | âŒ | âŒ |

</details>

## Documentation

- <a href="https://docs.rs/usls/latest/usls/">ğŸ“– <strong>API Documentation</strong></a>
-  <a href="./examples/README.md">ğŸŒŸ <strong>Examples</strong></a> 
-  <a href="./examples/README.md#how-to-use">ğŸŒŸ <strong>How to Use</strong></a> 

## ğŸ”§ Cargo Features

> â• Features in ***italics*** are enabled by default.

- ### Core & Utilities
  - ***`ort-download-binaries`***: Automatically download prebuilt ONNX Runtime binaries from [pyke](https://ort.pyke.io/perf/execution-providers).
  - **`ort-load-dynamic`**: Manually link ONNX Runtime. Useful for custom builds or unsupported platforms. See [Linking Guide](https://ort.pyke.io/setup/linking#static-linking) for more details.
  - **`viewer`**: Real-time image/video visualization (similar to OpenCV `imshow`). Empowered by [minifb](https://github.com/emoon/rust_minifb).
  - **`video`**: Video I/O support for reading and writing video streams. Empowered by [video-rs](https://github.com/oddity-ai/video-rs).
  - **`hf-hub`**: Download model files from Hugging Face Hub.
  - ***`annotator`***: Annotation utilities for drawing bounding boxes, keypoints, and masks on images.

- ### Image Formats
  Additional image format support (optional for faster compilation):
  
  - **`image-all-formats`**: Enable all additional image formats.
  - **`image-gif`**, **`image-bmp`**, **`image-ico`**, **`image-avif`**, **`image-tiff`**, **`image-dds`**, **`image-exr`**, **`image-ff`**, **`image-hdr`**, **`image-pnm`**, **`image-qoi`**, **`image-tga**: Individual image format support.

- ### Model Categories
  - ***`vision`***: Core vision models (Detection, Segmentation, Classification, Pose, etc.).
  - **`vlm`**: Vision-Language Models (CLIP, BLIP, Florence2, etc.).
  - **`mot`**: Multi-Object Tracking utilities.
  - **`all-models`**: Enable all model categories.

- ### Execution Providers
  Hardware acceleration for inference. Enable the one matching your hardware:

  - **`cuda`**: NVIDIA CUDA execution provider (pure model inference acceleration).
  - **`tensorrt`**: NVIDIA TensorRT execution provider (pure model inference acceleration).
  - **`nvrtx`**: NVIDIA NvTensorRT-RTX execution provider (pure model inference acceleration).
  - **`cuda-full`**: `cuda` + `cuda-runtime-build` (Model + Image Preprocessing acceleration).
  - **`tensorrt-full`**: `tensorrt` + `cuda-runtime-build` (Model + Image Preprocessing acceleration).
  - **`nvrtx-full`**: `nvrtx` + `cuda-runtime-build` (Model + Image Preprocessing acceleration).
  - **`coreml`**: Apple Silicon (macOS/iOS).
  - **`openvino`**: Intel CPU/GPU/VPU.
  - **`onednn`**: Intel Deep Neural Network Library.
  - **`directml`**: DirectML (Windows).
  - **`webgpu`**: WebGPU (Web/Chrome).
  - **`rocm`**: AMD GPU acceleration.
  - **`cann`**: Huawei Ascend NPU.
  - **`rknpu`**: Rockchip NPU.
  - **`xnnpack`**: Mobile CPU optimization.
  - **`acl`**: Arm Compute Library.
  - **`armnn`**: Arm Neural Network SDK.
  - **`azure`**: Azure ML execution provider.
  - **`migraphx`**: AMD MIGraphX.
  - **`nnapi`**: Android Neural Networks API.
  - **`qnn`**: Qualcomm SNPE.
  - **`tvm`**: Apache TVM.
  - **`vitis`**: Xilinx Vitis AI.

- ### CUDA Support
  NVIDIA GPU acceleration with CUDA image processing kernels (requires `cudarc`):

  - **`cuda-full`**: Uses `cuda-version-from-build-system` (auto-detects via `nvcc`).
  - **`cuda-11040`**, **`cuda-11050`**, **`cuda-11060`**, **`cuda-11070`**, **`cuda-11080`**: CUDA 11.x versions (Model + Preprocess).
  - **`cuda-12000`**, **`cuda-12010`**, **`cuda-12020`**, **`cuda-12030`**, **`cuda-12040`**, **`cuda-12050`**, **`cuda-12060`**, **`cuda-12080`**, **`cuda-12090`**: CUDA 12.x versions (Model + Preprocess).
  - **`cuda-13000`**, **`cuda-13010`**: CUDA 13.x versions (Model + Preprocess).

- ### TensorRT Support
  NVIDIA TensorRT execution provider with CUDA runtime libraries:

  - **`tensorrt-full`**: Uses `cuda-version-from-build-system` (auto-detects via `nvcc`).
  - **`tensorrt-cuda-11040`**, **`tensorrt-cuda-11050`**, **`tensorrt-cuda-11060`**, **`tensorrt-cuda-11070`**, **`tensorrt-cuda-11080`**: TensorRT + CUDA 11.x runtime.
  - **`tensorrt-cuda-12000`**, **`tensorrt-cuda-12010`**, **`tensorrt-cuda-12020`**, **`tensorrt-cuda-12030`**, **`tensorrt-cuda-12040`**, **`tensorrt-cuda-12050`**, **`tensorrt-cuda-12060`**, **`tensorrt-cuda-12080`**, **`tensorrt-cuda-12090`**: TensorRT + CUDA 12.x runtime.
  - **`tensorrt-cuda-13000`**, **`tensorrt-cuda-13010`**: TensorRT + CUDA 13.x runtime.

  > **Note**: `tensorrt-cuda-*` features enable **TensorRT execution provider** with CUDA runtime libraries for image processing. The "cuda" in the name refers to `cudarc` dependency.

- ### NVRTX Support
  NVIDIA NvTensorRT-RTX execution provider with CUDA runtime libraries:

  - **`nvrtx-full`**: Uses `cuda-version-from-build-system` (auto-detects via `nvcc`). 
  - **`nvrtx-cuda-11040`**, **`nvrtx-cuda-11050`**, **`nvrtx-cuda-11060`**, **`nvrtx-cuda-11070`**, **`nvrtx-cuda-11080`**: NVRTX + CUDA 11.x runtime.
  - **`nvrtx-cuda-12000`**, **`nvrtx-cuda-12010`**, **`nvrtx-cuda-12020`**, **`nvrtx-cuda-12030`**, **`nvrtx-cuda-12040`**, **`nvrtx-cuda-12050`**, **`nvrtx-cuda-12060`**, **`nvrtx-cuda-12080`**, **`nvrtx-cuda-12090`**: NVRTX + CUDA 12.x runtime.
  - **`nvrtx-cuda-13000`**, **`nvrtx-cuda-13010`**: NVRTX + CUDA 13.x runtime.

  > **Note**: `nvrtx-cuda-*` features enable **NVRTX execution provider** with CUDA runtime libraries for image processing. The "cuda" in the name refers to `cudarc` dependency.

---

## ğŸš€ Device Combination Guide

| Scenario | Model Device (`--device`) | Processor Device (`--processor-device`) | Required Features (`-F`) |
| :--- | :--- | :--- | :--- |
| **CPU Only** | `cpu` | `cpu` | `vision` (default) |
| **GPU Inference (Slow Preprocess)** | `cuda` | `cpu` | `cuda` |
| **GPU Inference (Fast Preprocess)** | `cuda` | `cuda` | `cuda-full` or `cuda-120xxx` |
| **TensorRT (Slow Preprocess)** | `tensorrt` | `cpu` | `tensorrt` |
| **TensorRT (Fast Preprocess)** | `tensorrt` | `cuda` | `tensorrt-full` or `tensorrt-cuda-120xxx` |

> âš ï¸ In multi-GPU environments (e.g., `cuda:0`, `cuda:1`), you **MUST** ensure that both `--device` and `--processor-device` use the **SAME GPU ID**. 

---

## â“ FAQ

- **ONNX Runtime Issues**: For ONNX Runtime related errors, please check the [ort](https://github.com/pykeio/ort) issues or [onnxruntime](https://github.com/microsoft/onnxruntime) issues.
- **Other Issues**: For other questions or bug reports, see [issues](https://github.com/jamjamjon/usls/issues) or open a new discussion.


#### âš ï¸ Compatibility Note

If you encounter linking errors with `__isoc23_strtoll` or similar glibc symbols, use the dynamic loading feature:

```bash
cargo run -F ort-load-dynamic --example
```

#### **Why no LM models?**

This project focuses on vision and VLM models under 1B parameters for efficient inference.  

Many high-performance inference engines already exist for LM/LLM models like vLLM.  

Pure text embedding models may be considered in future releases.

#### **How fast is it?**

Refer to YOLO performance benchmarks in the [Performance](#performance) section above.

This project uses multi-threading, SIMD, and CUDA hardware acceleration for optimization.

While vision models like YOLO and RFDETR are optimized, other models may need further interface and post-processing optimization. 


## ğŸ¤ Contributing

This is a personal project maintained in spare time, so progress on performance optimization and new model support may vary.

**We highly welcome PRs for model optimization!** If you have expertise in specific models and can help optimize their interfaces or post-processing, your contributions would be invaluable. Feel free to open an issue or submit a pull request for suggestions, bug reports, or new features.

## ğŸ™ Acknowledgments

- This project is built on top of [ort (ONNX Runtime for Rust)](https://github.com/pykeio/ort), which provides seamless Rust bindings for [ONNX Runtime](https://github.com/microsoft/onnxruntime). Special thanks to the `ort` maintainers.

- Special thanks to [@kadu-v](https://github.com/kadu-v) for the [jamtrack-rs](https://github.com/kadu-v/jamtrack-rs) project, which inspired our ByteTracker implementation


Thanks to all the open-source libraries and their maintainers that make this project possible. See [Cargo.toml](Cargo.toml) for a complete list of dependencies.

## ğŸ“œ License

This project is licensed under [LICENSE](LICENSE).
