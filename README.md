<h2 align="center">usls</h2>
<p align="center">
<a href="https://github.com/jamjamjon/usls/actions/workflows/rust-ci.yml">
        <img src="https://github.com/jamjamjon/usls/actions/workflows/rust-ci.yml/badge.svg" alt="Rust CI">
    </a>
    <a href='https://crates.io/crates/usls'>
        <img src='https://img.shields.io/crates/v/usls?logo=rust&logoColor=white' alt='Crates.io Version'>
    </a>
    <a href='https://github.com/microsoft/onnxruntime/releases'>
        <img src='https://img.shields.io/badge/onnxruntime-%3E%3D%201.22.0-3399FF?logo=onnx&logoColor=white' alt='ONNXRuntime MSRV'>
    </a>
    <a href='https://crates.io/crates/usls'>
        <img src='https://img.shields.io/crates/msrv/usls?color=yellow&logo=rust' alt='Rust MSRV'>
    </a>
</p>


<br/>

<p align="center">
  <a href="https://docs.rs/usls/latest/usls/">ğŸ“˜ <strong>API Documentation</strong></a> | 
  <a href="./examples/README.md">ğŸŒŸ <strong>Examples</strong></a> |  
  <a href="#-model-zoo">ğŸ“¦ <strong>Model Zoo</strong></a>
</p>

<!-- --- -->

<br/>

**usls** is a cross-platform Rust library powered by ONNX Runtime for efficient inference of SOTA vision and vision-language models (***typically under 1B parameters***).

<p align="center">
<img src='https://github.com/jamjamjon/assets/releases/download/images/pipeline.png'>
(Generated by Seedream4.5)
</p>


## ğŸŒŸ Highlights

- **âš¡ High Performance**: Multi-threading, SIMD, and CUDA-accelerated processing
- **ğŸŒ Cross-Platform**: Linux, macOS, Windows with ONNX Runtime execution providers (CUDA, TensorRT, CoreML, OpenVINO, DirectML, etc.)
- **ğŸ—ï¸ Unified API**: Single `Model` trait inference with `run()`/`forward()`/`encode_images()`/`encode_texts()` and unified `Y` output
- **ğŸ“¥ Auto-Management**: Automatic model download (HuggingFace/GitHub), caching and path resolution
- **ğŸ“¦ Multiple Inputs**: Image, directory, video, webcam, stream and combinations
- **ğŸ› ï¸ Full-Stack Suite**: `DataLoader`, `Annotator`, and `Viewer` for complete workflows
- **ğŸŒ± Model Ecosystem**: 50+ SOTA vision and VLM models

## ğŸš€ Quick Start

Run the **YOLO-Series demo** to explore models with different tasks, precision and execution providers:

- **Tasks**: `detect`, `segment`, `pose`, `classify`, `obb`
- **Versions**: `YOLOv5`, `YOLOv6`, `YOLOv7`, `YOLOv8`, `YOLOv9`, `YOLOv10`, `YOLO11`, `YOLOv12`, `YOLOv13`
- **Scales**: `n`, `s`, `m`, `l`, `x`
- **Precision**: `fp32`, `fp16`, `q8`, `q4`, `q4f16`, `bnb4`
- **Execution Providers**: `CPU`, `CUDA`, `TensorRT`, `CoreML`, `OpenVINO`, and more

```bash
# CPU: Object detection, YOLOv8n, FP16
cargo run -r --example yolo -- --task detect --ver 8 --scale n --dtype fp16

# NVIDIA CUDA: Instance segmentation, YOLO11m
cargo run -r -F cuda --example yolo -- --task segment --ver 11 --scale m --device cuda:0 --processor-device cuda:0

# NVIDIA TensorRT
cargo run -r -F tensorrt --example yolo -- --device tensorrt:0 --processor-device cuda:0

# Apple Silicon CoreML
cargo run -r -F coreml --example yolo -- --device coreml

# Intel OpenVINO: CPU/GPU/VPU acceleration
cargo run -r -F openvino -F ort-load-dynamic --example yolo -- --device openvino:CPU

# Show all available options
cargo run -r --example yolo -- --help
```

> See [YOLO Examples](./examples/yolo/README.md) for more details and use cases.

### Performance

>**Environment:** NVIDIA RTX 3060Ti (TensorRT v10.11.0.33, CUDA 12.8) / Intel i5-12400F  
>
>**Setup:** YOLOv8n-Det on 5000 coco2017val images

#### Batch = 1

| Backend | DType | Preprocess | Inference | Postprocess | Total |
| --- | --- | --- | --- | --- | --- |
| **TensorRT** | FP32 | 330.870Âµs | 1.829ms | 610.023Âµs | 2.770ms |
| **CUDA** | FP32 | 245.181Âµs | 4.143ms | 433.166Âµs | 4.877ms |
| **CUDA** | FP16 | 324.337Âµs | 3.753ms | 401.215Âµs | 4.479ms |
| **CPU** | FP32 | 5.751ms | 41.825ms | 2.274ms | 49.850ms |
| **CPU** | FP16 | 5.830ms | 55.281ms | 2.397ms | 63.508ms |

#### Batch = 8

| Backend | DType | Preprocess | Inference | Postprocess | Total |
| --- | --- | --- | --- | --- | --- |
| **TensorRT** | FP32 | 1.660ms | 10.001ms | 1.565ms | 13.226ms |
| **CUDA** | FP32 | 2.104ms | 24.367ms | 1.419ms | 27.89ms |
| **CUDA** | FP16 | 1.600ms | 19.705ms | 1.517ms | 22.822ms |
| **CPU** | FP32 | 19.970ms | 396.454ms | 6.961ms | 423.385ms |
| **CPU** | FP16 | 19.952ms | 466.687ms | 7.200ms | 493.839ms |




## ğŸ“¦ Model Zoo

> **Status:**â€‚âœ… **Supported**â€‚â€‚|â€‚â€‚â“ **Unknown**â€‚â€‚|â€‚â€‚âŒ **Not Supported For Now**

***TODO: Update the status***

<details closed>
<summary><b>ğŸ”¥ YOLO-Series</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [YOLOv5](https://github.com/ultralytics/yolov5) | Image Classification<br />Object Detection<br />Instance Segmentation | [demo](./examples/yolo) | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | 
| [YOLOv6](https://github.com/meituan/YOLOv6) | Object Detection | [demo](./examples/yolo) | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | 
| [YOLOv7](https://github.com/WongKinYiu/yolov7) | Object Detection | [demo](./examples/yolo) | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | 
| [YOLOv8](https://github.com/ultralytics/ultralytics) | Object Detection<br />Instance Segmentation<br />Image Classification<br />Oriented Object Detection<br />Keypoint Detection | [demo](./examples/yolo) | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | 
| [YOLO11](https://github.com/ultralytics/ultralytics) | Object Detection<br />Instance Segmentation<br />Image Classification<br />Oriented Object Detection<br />Keypoint Detection | [demo](./examples/yolo) | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | 
| [YOLOv9](https://github.com/WongKinYiu/yolov9) | Object Detection | [demo](./examples/yolo) | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | 
| [YOLOv10](https://github.com/THU-MIG/yolov10) | Object Detection | [demo](./examples/yolo) | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | 
| [YOLOv12](https://github.com/sunsmarterjie/yolov12) | Image Classification<br />Object Detection<br />Instance Segmentation | [demo](./examples/yolo) | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | 
| [YOLOv13](https://github.com/iMoonLab/yolov13) | Object Detection | [demo](./examples/yolo) | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | 

</details>


<details closed>
<summary><b>ğŸ·ï¸ Image Classification & Tagging</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [BEiT](https://github.com/microsoft/unilm/tree/master/beit) | Image Classification | [demo](./examples/image-classification) | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | 
| [ConvNeXt](https://github.com/facebookresearch/ConvNeXt) | Image Classification | [demo](./examples/image-classification) | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | 
| [FastViT](https://github.com/apple/ml-fastvit) | Image Classification | [demo](./examples/image-classification) | âœ… | âœ…| âœ… | âœ…| âŒ | âŒ | âŒ | 
| [MobileOne](https://github.com/apple/ml-mobileone) | Image Classification | [demo](./examples/image-classification) | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | 
| [DeiT](https://github.com/facebookresearch/deit) | Image Classification | [demo](./examples/image-classification) | âœ… | âœ… | âœ… | âœ…| âŒ | âŒ | âŒ | 
| [RAM](https://github.com/xinyu1205/recognize-anything) | Image Tagging | [demo](./examples/image-classification) | âœ… | â“| âœ… | âœ…| âœ… | âœ… | âœ… |
| [RAM++](https://github.com/xinyu1205/recognize-anything) | Image Tagging | [demo](./examples/image-classification) | âœ… | â“ | âœ… | âœ… | âœ… | âœ… | âœ… |

</details>


<details closed>
<summary><b>ğŸ¯ Object Detection</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [RT-DETRv1](https://github.com/lyuwenyu/RT-DETR) | Object Detection | [demo](./examples/object-detection) | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | 
| [RT-DETRv2](https://github.com/lyuwenyu/RT-DETR) | Object Detection | [demo](./examples/object-detection) | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ…  | 
| [RT-DETRv4](https://github.com/RT-DETRs/RT-DETRv4) | Object Detection | [demo](./examples/object-detection) | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ…  | 
| [RF-DETR](https://github.com/roboflow/rf-detr) | Object Detection | [demo](./examples/object-detection)| âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | 
| [PP-PicoDet](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.8/configs/picodet) | Object Detection | [demo](./examples/ocr) | âŒ | â“  | âœ… | âŒ | âŒ | âŒ | âŒ | 
| [D-FINE](https://github.com/manhbd-22022602/D-FINE) | Object Detection | [demo](./examples/object-detection) | âœ… | â“  | âœ…  | âŒ | âŒ | âŒ | âŒ | âŒ | 
| [DEIM](https://github.com/ShihuaHuang95/DEIM) | Object Detection | [demo](./examples/object-detection) | âœ… | â“  | âœ…  | âŒ | âŒ | âŒ | âŒ | âŒ | 
| [DEIMv2](https://github.com/Intellindust-AI-Lab/DEIMv2) | Object Detection | [demo](./examples/object-detection) | âœ… | â“  |âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | 

</details>

<details closed>
<summary><b>ğŸ¨ Image Segmentation</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [SAM](https://github.com/facebookresearch/segment-anything) | Segment Anything | [demo](./examples/image-segmentation) | âœ… | â“ | âœ… | âŒ | âŒ | âŒ | âŒ |  
| [SAM-HQ](https://github.com/SysCV/sam-hq) | Segment Anything | [demo](./examples/image-segmentation) | âœ… | â“ | âœ… | âŒ | âŒ | âŒ | âŒ | 
| [MobileSAM](https://github.com/ChaoningZhang/MobileSAM) | Segment Anything | [demo](./examples/image-segmentation) | âœ… |â“ | âœ… | âŒ | âŒ | âŒ | âŒ |  
| [EdgeSAM](https://github.com/chongzhou96/EdgeSAM) | Segment Anything | [demo](./examples/image-segmentation) | âœ… | â“ | âœ… | âŒ | âŒ | âŒ | âŒ | 
| [FastSAM](https://github.com/CASIA-IVA-Lab/FastSAM) | Instance Segmentation | [demo](./examples/image-segmentation) | âœ… | âœ… | âœ…  | âœ…  | âœ…  | âœ…  | âœ…  |
| [SAM2](https://github.com/facebookresearch/segment-anything-2) | Segment Anything | [demo](./examples/image-segmentation) | âœ… | â“ | âœ… | âŒ | âŒ  | âŒ | âŒ |  
| [SAM3-Tracker](https://github.com/facebookresearch/segment-anything-3) | Segment Anything | [demo](./examples/image-segmentation) | âœ… | âœ… | âœ…  | âœ…  | âœ…  | âœ…  | âœ…  |

</details>

<details closed>
<summary><b>ğŸ—ºï¸ Open-Set Detection & Segmentation</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [GroundingDINO](https://github.com/IDEA-Research/GroundingDINO) | Open-Set Detection With Language | [demo](./examples/open-set-detection) | âœ… | â“ | âœ…  | âœ…  | âœ…  | âœ…  | âœ…  |
| [MM-GDINO](https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/README.md) | Open-Set Detection With Language | [demo](./examples/open-set-detection) | âœ… | â“ | âœ…  | âœ…  | âœ…  | âœ…  | âœ…  |
| [LLMDet](https://github.com/iSEE-Laboratory/LLMDet) | Open-Set Detection With Language | [demo](./examples/open-set-detection) | âœ… | â“ | âœ…  | âœ…  | âœ…  | âœ…  | âœ…  |
| [OWLv2](https://huggingface.co/google/owlv2-base-patch16-ensemble) | Open-Set Object Detection | [demo](./examples/open-set-detection) | âœ… | â“  | âœ… | âœ…   | âŒ | âŒ | âŒ |
| [YOLO-World](https://github.com/AILab-CVC/YOLO-World) | Open-Set Detection With Language | [demo](./examples/yolo) | âœ… | âœ… |âœ…  | âœ…  | âœ…  | âœ…  | âœ…  |
| [YOLOE](https://github.com/THU-MIG/yoloe) | Open-Set Detection And Segmentation | [demo](./examples/open-set-segmentation) | âœ… | âœ… | âœ…  | âœ…  | âœ…  | âœ…  | âœ…  |
| [SAM3-Image](https://github.com/facebookresearch/segment-anything-3) | Open-Set Detection And Segmentation| [demo](./examples/open-set-segmentation) | âœ… | âœ… | âœ…  | âœ…  | âœ…  | âœ…  | âœ…  |

</details>


<details closed>
<summary><b>âœ¨ Background Removal</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [RMBG](https://huggingface.co/briaai/RMBG-2.0) | Image Segmentation<br />Background Removal | [demo](./examples/background-removal) | âœ… | â“ |âœ…  | âœ…  | âœ…  | âœ…  | âœ…  |
| [BEN2](https://huggingface.co/PramaLLC/BEN2) | Image Segmentation<br />Background Removal | [demo](./examples/background-removal) | âœ… | â“ |âœ…  | âœ…  | âŒ | âŒ | âŒ |

</details>

<details closed>
<summary><b>ğŸƒ Multi-Object Tracking</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [ByteTrack](https://github.com/FoundationVision/ByteTrack) | Multi-Object Tracking | [demo](./examples/mot) | âŒ | âŒ  | âŒ | âŒ | âŒ | âŒ | âŒ |

</details>



<details closed>
<summary><b>ğŸ’ Image Super-Resolution</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [Swin2SR](https://github.com/mv-lab/swin2sr) | Image Restoration | [demo](./examples/super-resolution) | âœ… |  â“ | âœ…  | âœ…  | âœ…  | âœ…  | âœ…  |
| [APISR](https://github.com/Kiteretsu77/APISR) | Anime Super-Resolution | [demo](./examples/super-resolution) | âœ… |  â“ | âœ…  | âœ…  | âœ…  | âœ…  | âœ…  |

</details>

<details closed>
<summary><b>âœ‚ï¸ Image Matting</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [MODNet](https://github.com/ZHKKKe/MODNet) | Image Matting | [demo](./examples/image-matting) | âœ… |  â“ | âœ…  | âœ…  | âœ…  | âŒ  | âŒ  |
| [MediaPipe Selfie](https://ai.google.dev/edge/mediapipe/solutions/vision/image_segmenter) | Image Segmentation | [demo](./examples/image-matting) | âœ… |  â“ | âœ…  | âœ…  | âœ…  | âŒ  | âŒ  |

</details>

<details closed>
<summary><b>ğŸ¤¸ Pose Estimation</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [RTMPose](https://github.com/open-mmlab/mmpose/tree/dev-1.x/projects/rtmpose) | Keypoint Detection | [demo](./examples/pose-estimation) | âœ… | â“ |  âœ…  | âœ…  | âœ…  | âœ…  | âœ…  |
| [DWPose](https://github.com/IDEA-Research/DWPose) | Keypoint Detection | [demo](./examples/pose-estimation) | âœ… | â“ |  âœ…  | âœ…  | âœ…  | âœ…  | âœ…  |
| [RTMW](https://arxiv.org/abs/2407.08634) | Keypoint Detection | [demo](./examples/pose-estimation) | âœ… | â“ |  âœ…  | âœ…  | âœ…  | âœ…  | âœ…  |
| [RTMO](https://github.com/open-mmlab/mmpose/tree/main/projects/rtmo) | Keypoint Detection | [demo](./examples/pose-estimation) | âœ… | â“ |  âœ…  | âœ…  | âœ…  | âœ…  | âŒ  |

</details>

<details closed>
<summary><b>ğŸ” OCR & Document Understanding</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [DB](https://arxiv.org/abs/1911.08947) | Text Detection | [demo](./examples/ocr) | âœ… |  â“ | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ |
| [FAST](https://github.com/czczup/FAST) | Text Detection | [demo](./examples/ocr) | âœ… |  â“ | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ |
| [LinkNet](https://arxiv.org/abs/1707.03718) | Text Detection | [demo](./examples/ocr) | âœ… |  â“ | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ |
| [SVTR](https://arxiv.org/abs/2205.00159) | Text Recognition | [demo](./examples/ocr) | âœ… | â“ | âœ… | âœ… | âŒ | âŒ | âŒ |
| [TrOCR](https://huggingface.co/microsoft/trocr-base-printed) | Text Recognition | [demo](./examples/ocr) | âœ… | â“ | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ |
| [SLANet](https://paddlepaddle.github.io/PaddleOCR/latest/algorithm/table_recognition/algorithm_table_slanet.html) | Table Recognition | [demo](./examples/ocr) | âœ… | â“ | âœ… | âœ… | âŒ | âŒ | âŒ |
| [DocLayout-YOLO](https://github.com/opendatalab/DocLayout-YOLO) | Object Detection | [demo](./examples/ocr) | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ |

</details>

<details closed>
<summary><b>ğŸ§© Vision-Language Models (VLM)</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [BLIP](https://github.com/salesforce/BLIP) | Image Captioning | [demo](./examples/vlm) | âœ… | â“ | âœ… |â“ | âŒ | âŒ | âŒ |
| [Florence2](https://arxiv.org/abs/2311.06242) | A Variety of Vision Tasks | [demo](./examples/vlm) | âœ… | â“ | âœ… |âœ…  | âŒ | âŒ | âŒ |
| [Moondream2](https://github.com/vikhyat/moondream/tree/main) | Open-Set Object Detection<br />Open-Set Keypoints Detection<br />Image Captioning<br />Visual Question Answering | [demo](./examples/vlm) | âœ… | â“  | âŒ | âŒ |âœ…  | âœ…  | âŒ |
| [SmolVLM](https://huggingface.co/HuggingFaceTB/SmolVLM-256M-Instruct) | Visual Question Answering | [demo](./examples/vlm) | âœ… | â“| âœ… | â“ | â“ | â“ | â“ | 
| [SmolVLM2](https://huggingface.co/HuggingFaceTB/SmolVLM-256M-Instruct) | Visual Question Answering | [demo](./examples/vlm) | âœ… | â“| âœ… | â“ | â“ | â“ | â“ | 
| [FastVLM](https://github.com/apple/ml-fastvlm) | Vision Language Models | [demo](./examples/vlm) | âœ… |  â“ | âœ…  | âœ…|âœ…  | âœ… | âœ… |

</details>


<details closed>
<summary><b>ğŸ§¬ Embedding Model</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [CLIP](https://github.com/openai/CLIP) | Vision-Language Embedding | [demo](./examples/embedding) | âœ… | â“ | âœ…  | âœ…|âœ…  | âœ… | âœ… |
| [jina-clip-v1](https://huggingface.co/jinaai/jina-clip-v1) | Vision-Language Embedding | [demo](./examples/embedding) | âœ… | â“ | âœ…  | âœ…|âœ…  | âœ… | âœ… |
| [jina-clip-v2](https://huggingface.co/jinaai/jina-clip-v2) | Vision-Language Embedding | [demo](./examples/embedding) | âœ… | â“ | âœ…  | âœ…|âœ…  | âœ… | âœ… |
| [mobileclip](https://github.com/apple/ml-mobileclip) | Vision-Language Embedding | [demo](./examples/embedding) | âœ… | â“ | âœ…  | âœ…|âœ…  | âœ… | âœ… |
| [DINOv2](https://github.com/facebookresearch/dinov2) | Vision Embedding | [demo](./examples/embedding) | âœ… | â“ | âœ…  | âŒ|âŒ  | âŒ | âŒ |
| [DINOv3](https://github.com/facebookresearch/dinov3) | Vision Embedding | [demo](./examples/embedding) | âœ… | â“ | âœ…  | âœ…|âœ…  | âœ… | âœ… |
</details>

<details closed>
<summary><b>ğŸ“ Depth Estimation</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [DepthAnything v1](https://github.com/LiheYoung/Depth-Anything) | Monocular Depth Estimation | [demo](./examples/depth-estimation) | âœ… | â“ | âœ…  | âœ…|âœ…  | âœ… | âœ… |
| [DepthAnything v2](https://github.com/LiheYoung/Depth-Anything) | Monocular Depth Estimation | [demo](./examples/depth-estimation) | âœ… |  â“ | âœ…  | âœ…|âœ…  | âœ… | âœ… |
| [DepthPro](https://github.com/apple/ml-depth-pro) | Monocular Depth Estimation | [demo](./examples/depth-estimation) | âœ… | â“ | âœ…  | âœ…|âœ…  | âœ… | âœ… |
| [Depth-Anything-3](https://github.com/ByteDance-Seed/Depth-Anything-3) | Monocular<br/>Metric<br/>Multi-View | [demo](./examples/depth-estimation) | âœ… | â“ | âœ…  | âœ…|âœ…  | âœ… | âœ… |

</details>



<details closed>
<summary><b>ğŸŒŒ Others</b></summary>

| Model | Task / Description | Demo | Dynamic Batch | TensorRT | FP32 | FP16 | Q8 | Q4f16 | BNB4 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| [Sapiens](https://github.com/facebookresearch/sapiens/tree/main) | Foundation for Human Vision Models | [demo](./examples/sapiens) | âœ… | â“ | âœ…  | âœ…|âœ…  | âœ… | âœ… |
| [YOLOPv2](https://arxiv.org/abs/2208.11434) | Panoptic Driving | [demo](./examples/image-segmentation) | âœ… | â“ | âœ… | âŒ| âŒ  | âŒ | âŒ |

</details>

## Documentation

- <a href="https://docs.rs/usls/latest/usls/">ğŸ“– <strong>API Documentation</strong></a>
-  <a href="./examples/README.md">ğŸŒŸ <strong>Examples</strong></a> 
-  <a href="./examples/README.md#how-to-use">ğŸŒŸ <strong>How to Use</strong></a> 

## ğŸ”§ Cargo Features

> â• Features in ***italics*** are enabled by default.

- ### Core & Utilities
  - ***`ort-download-binaries`***: Automatically download prebuilt ONNX Runtime binaries from [pyke](https://ort.pyke.io/perf/execution-providers).
  - **`ort-load-dynamic`**: Manually link ONNX Runtime. Useful for custom builds or unsupported platforms. See [Linking Guide](https://ort.pyke.io/setup/linking#static-linking) for more details.
  - **`viewer`**: Real-time image/video visualization (similar to OpenCV `imshow`). Empowered by [minifb](https://github.com/emoon/rust_minifb).
  - **`video`**: Video I/O support for reading and writing video streams. Empowered by [video-rs](https://github.com/oddity-ai/video-rs).
  - **`hf-hub`**: Download model files from Hugging Face Hub.
  - ***`annotator`***: Annotation utilities for drawing bounding boxes, keypoints, and masks on images.

- ### Image Formats
  Additional image format support (optional for faster compilation):
  
  - **`image-all-formats`**: Enable all additional image formats.
  - **`image-gif`**, **`image-bmp`**, **`image-ico`**, **`image-avif`**, **`image-tiff`**, **`image-dds`**, **`image-exr`**, **`image-ff`**, **`image-hdr`**, **`image-pnm`**, **`image-qoi`**, **`image-tga**: Individual image format support.

- ### Model Categories
  - ***`vision`***: Core vision models (Detection, Segmentation, Classification, Pose, etc.).
  - **`vlm`**: Vision-Language Models (CLIP, BLIP, Florence2, etc.).
  - **`mot`**: Multi-Object Tracking utilities.
  - **`all-models`**: Enable all model categories.

- ### Execution Providers
  Hardware acceleration for inference. Enable the one matching your hardware:

  - **`cuda`**, **`tensorrt`**: NVIDIA GPU acceleration (includes CUDA image processing).
  - **`coreml`**: Apple Silicon (macOS/iOS).
  - **`openvino`**: Intel CPU/GPU/VPU.
  - **`onednn`**: Intel Deep Neural Network Library.
  - **`directml`**: DirectML (Windows).
  - **`webgpu`**: WebGPU (Web/Chrome).
  - **`rocm`**: AMD GPU acceleration.
  - **`cann`**: Huawei Ascend NPU.
  - **`rknpu`**: Rockchip NPU.
  - **`xnnpack`**: Mobile CPU optimization.
  - **`acl`**: Arm Compute Library.
  - **`armnn`**: Arm Neural Network SDK.
  - **`azure`**: Azure ML execution provider.
  - **`migraphx`**: AMD MIGraphX.
  - **`nnapi`**: Android Neural Networks API.
  - **`qnn`**: Qualcomm SNPE.
  - **`tvm`**: Apache TVM.
  - **`vitis`**: Xilinx Vitis AI.

- ### CUDA Support
  NVIDIA GPU acceleration with CUDA image processing kernels:

  - **`cuda`**: Uses `cuda-version-from-build-system` (auto-detects via `nvcc`).
  - **`cuda-11040`**, **`cuda-11050`**, **`cuda-11060`**, **`cuda-11070`**, **`cuda-11080`**: CUDA 11.x specific versions.
  - **`cuda-12000`**, **`cuda-12010`**, **`cuda-12020`**, **`cuda-12030`**, **`cuda-12040`**, **`cuda-12050`**, **`cuda-12060`**, **`cuda-12080`**, **`cuda-12090`**: CUDA 12.x specific versions.
  - **`cuda-13000`**, **`cuda-13010`**: CUDA 13.x specific versions.

  See [ONNX Runtime docs](https://onnxruntime.ai/docs/execution-providers/) and [ORT performance guide](https://ort.pyke.io/perf/execution-providers) for details.


## â“ FAQ

- **ONNX Runtime Issues**: For ONNX Runtime related errors, please check the [ort](https://github.com/pykeio/ort) issues or [onnxruntime](https://github.com/microsoft/onnxruntime) issues.
- **Other Issues**: For other questions or bug reports, see [issues](https://github.com/jamjamjon/usls/issues) or open a new discussion.


#### **Why no LM models?**

This project focuses on vision and VLM models under 1B parameters for efficient inference.  

Many high-performance inference engines already exist for LM/LLM models like vLLM.  

Pure text embedding models may be considered in future releases.

#### **How fast is it?**

Refer to YOLO performance benchmarks in the [Performance](#performance) section above.

This project uses multi-threading, SIMD, and CUDA hardware acceleration for optimization.

While vision models like YOLO and RFDETR are optimized, other models may need further interface and post-processing optimization. 


## ğŸ¤ Contributing

This is a personal project maintained in spare time, so progress on performance optimization and new model support may vary.

**We highly welcome PRs for model optimization!** If you have expertise in specific models and can help optimize their interfaces or post-processing, your contributions would be invaluable. Feel free to open an issue or submit a pull request for suggestions, bug reports, or new features.

## ğŸ™ Acknowledgments

This project is built on top of [ort (ONNX Runtime for Rust)](https://github.com/pykeio/ort), which provides seamless Rust bindings for [ONNX Runtime](https://github.com/microsoft/onnxruntime). Special thanks to the `ort` maintainers.

Thanks to all the open-source libraries and their maintainers that make this project possible. See [Cargo.toml](Cargo.toml) for a complete list of dependencies.

## ğŸ“œ License

This project is licensed under [LICENSE](LICENSE).